{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Final ^^ .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dq89fB_d3ymW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV5xB10LID7K",
        "outputId": "a70201b3-99c7-4120-9006-23d51919aba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLmNE-EJra_3"
      },
      "source": [
        "# **Collab code to mount drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MfH1Jl4UpHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d56e70b-ccb5-40d3-94c6-569ee83298ec"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "#vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Cannot retrieve auth tokens.\n",
            "Failure(\"Unexpected error response: {\\n  \\\"error\\\": \\\"invalid_client\\\",\\n  \\\"error_description\\\": \\\"The OAuth client was not found.\\\"\\n}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmra-t6UUKX"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xCQ5O9IW6_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7277d739-4d99-4edf-ed1f-0d31758fc8f0"
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Error: Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-filq0EKtt8h"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# **Loading libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsKlSXiLUUKc"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import pickle\n",
        "from torch.autograd import Variable\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjHdIj5J3JZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFBv3ukIvbUo"
      },
      "source": [
        "*Load* & Preprocess Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfkoW7YTvbUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33804968-bdaa-486a-ab70-e57d00673bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
            "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
            "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
            "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
            "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
            "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
            "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
            "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
            "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
            "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
          ]
        }
      ],
      "source": [
        "corpus_name = \"/content/drive/MyDrive/Datasets/ALLDataset\"\n",
        "corpus = os.path.join(\"data\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"movie_lines.txt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YriD6ny_vbUp"
      },
      "source": [
        " Create formatted data file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FllAKhEbvbUq"
      },
      "outputs": [],
      "source": [
        "# Splits each line of the file into a dictionary of fields\n",
        "def loadLines(fileName, fields):\n",
        "    lines = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\" +++$+++ \")\n",
        "            # Extract fields\n",
        "            lineObj = {}\n",
        "            for i, field in enumerate(fields):\n",
        "                lineObj[field] = values[i]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
        "def loadConversations(fileName, lines, fields):\n",
        "    conversations = []\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\" +++$+++ \")\n",
        "            # Extract fields\n",
        "            convObj = {}\n",
        "            for i, field in enumerate(fields):\n",
        "                convObj[field] = values[i]\n",
        "            utterance_id_pattern = re.compile('L[0-9]+')\n",
        "            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n",
        "            # Reassemble lines\n",
        "            convObj[\"lines\"] = []\n",
        "            for lineId in lineIds:\n",
        "                convObj[\"lines\"].append(lines[lineId])\n",
        "            conversations.append(convObj)\n",
        "    return conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9pT8tFMvbUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a900e4-4766-40e0-dbb7-55cca7d39658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus...\n",
            "\n",
            "Loading conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
            "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
            "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
          ]
        }
      ],
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines2.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Initialize lines dict, conversations list, and field ids\n",
        "lines = {}\n",
        "conversations = []\n",
        "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
        "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
        "\n",
        "# Load lines and process conversations\n",
        "print(\"\\nProcessing corpus...\")\n",
        "lines = loadLines(os.path.join(corpus, \"/content/drive/MyDrive/Datasets/ALLDataset/movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "print(\"\\nLoading conversations...\")\n",
        "conversations = loadConversations(os.path.join(corpus, \"/content/drive/MyDrive/Datasets/ALLDataset/movie_conversations.txt\"),\n",
        "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38aiWJ1mdbAa"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# **Vocabulary Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xVqfoSuUUKf"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self):        \n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuPff2jpdfSl"
      },
      "source": [
        "# **Custom functions to preprocess Strings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "uVE7--cVhfsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3793478-814b-42ab-8cf9-b26603810826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') \n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YD84vUShc1J",
        "outputId": "ca58e663-cd32-4476-b7ea-7e53b012a363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS\n",
        "'''\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        " \n",
        "str= '''My name is Tony Stark and I am Iron Man. '''\n",
        "words = word_tokenize(str)\n",
        "postag = pos_tag(words)\n",
        "print(postag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl2QysMn2Sis",
        "outputId": "c8063820-7e9f-48d4-df97-db0ee46c3c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Tony', 'NNP'), ('Stark', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('Iron', 'NNP'), ('Man', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y3UjcoRr2SWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS\n",
        "'''\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1OnVhJLySH_",
        "outputId": "c3b4df09-18a2-4dcc-ad3d-414a2f6d43a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS\n",
        "'''\n",
        "\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "kzwWcmNyx7Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
        "\n",
        "\n",
        "\n",
        "sentence = \"you know what i mean\"\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
        "\n",
        "sentence =\"william - he asked me to meet him here.\"\n",
        "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0T917ii5AqZ",
        "outputId": "88c88615-a1f3-44c0-9c4e-5955f9141e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n",
            "['you', 'know', 'what', 'i', 'mean']\n",
            "['william', '-', 'he', 'ask', 'me', 'to', 'meet', 'him', 'here', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0c7IjGLUUKj"
      },
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
        "\n",
        "# Python program to convert a list to string\n",
        "# Function to convert  \n",
        "def listToString(s): \n",
        "    # initialize an empty string\n",
        "    str1 = \"\" \n",
        "    # traverse in the string  \n",
        "    for ele in s: \n",
        "        str1 += ele \n",
        "        str1 +=' ' \n",
        "    # return string  \n",
        "    return str1\n",
        "\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    #print(\"s   \",s)\n",
        "    s = re.sub(r\"i'm\", r\"i am\", s)\n",
        "    s = re.sub(r\"he's\", r\"he is\", s)\n",
        "    s = re.sub(r\"she's\", r\"she is\", s)\n",
        "    s = re.sub(r\"it's\", r\"it is\", s)\n",
        "    s = re.sub(r\"that's\", r\"that is\", s)\n",
        "    s = re.sub(r\"what's\", r\"what is\", s)\n",
        "    s = re.sub(r\"where's\", r\"where is\", s)\n",
        "    s = re.sub(r\"\\'s\", r\" is\", s)\n",
        "    s = re.sub(r\"\\'ll\", r\" will\", s)\n",
        "    s = re.sub(r\"\\'ve\", r\" have\", s)\n",
        "    s = re.sub(r\"\\'re\", r\" are\", s)\n",
        "    s = re.sub(r\"\\'d\", r\" would\", s)\n",
        "    s = re.sub(r\"won't\", r\"will not\", s)\n",
        "    s = re.sub(r\"don't\", r\"do not\", s)\n",
        "    s = re.sub(r\"didn't\", r\"did not\", s)\n",
        "    s = re.sub(r\"doesn't\", r\"does not\", s)\n",
        "    s = re.sub(r\"can't\", r\"can not\", s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    s = re.sub(r\"[-()\\\"“”:)#/@;:<>{}+=~|.?,]😊🙂☺️❤️❓.\", r\"\", s)\n",
        "    word_list = nltk.word_tokenize(s)\n",
        "    #print(word_list)\n",
        "    lemmatizer = WordNetLemmatizer()   \n",
        "    # an instance of Word Net Lemmatizer\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_list] \n",
        "    s = listToString(lemmatized_words)\n",
        "    #print('*******************')\n",
        "\n",
        "    return s\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile):\n",
        "    print(\"Reading lines...\")    \n",
        "    # Read the file and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc()\n",
        "    return voc, pairs\n",
        "\n",
        "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filter pairs using filterPair condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(datafile):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N26s_MXKdqfV"
      },
      "source": [
        "# **Reading chat file**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaKXKfuFhrUR",
        "outputId": "0b19d3c6-ee10-4843-c49a-06e8d5142795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ8iJn6sd2rG",
        "outputId": "482ccbb1-f56f-4a35-f2ae-154e6304aabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load/Assemble voc and pairs\n",
        "\n",
        "datafile = '/content/drive/MyDrive/Datasets/ALLDataset/formatted_movie_lines2.txt'\n",
        "voc, pairs = loadPrepareData(datafile)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 52667 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 13363\n",
            "\n",
            "pairs:\n",
            "['there . ', 'where ? ']\n",
            "['you have my word . a a gentleman ', 'you be sweet . ']\n",
            "['hi . ', 'look like thing work out tonight huh ? ']\n",
            "['you know chastity ? ', 'i believe we share an art instructor ']\n",
            "['have fun tonight ? ', 'ton ']\n",
            "['but ', 'you always be this selfish ? ']\n",
            "['do you listen to this crap ? ', 'what crap ? ']\n",
            "['what good stuff ? ', 'the real you . ']\n",
            "['the real you . ', 'like my fear of wear pastel ? ']\n",
            "['wow ', 'let be go . ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haHlxJgqUUKo"
      },
      "source": [
        "\n",
        "# **2) Filter out pairs with trimmed words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLJUsY8SUUKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50d0d2f-4e14-484e-86c8-3ae33b538253"
      },
      "source": [
        "#input_variable\n",
        "\n",
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 5883 / 13360 = 0.4403\n",
            "Trimmed from 52667 pairs to 44385, 0.8427 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMiPPAPCz7Bs"
      },
      "source": [
        "# **Splitting testing and training pairs**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpymeeyjz_Ks"
      },
      "source": [
        "#testing pairs\n",
        "testpairs = pairs[450000:]\n",
        "\n",
        "#Training pairs\n",
        "pairs  = pairs[:45000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7HNHHqIepTv"
      },
      "source": [
        "# **Generating batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjbTwFVxUUKt"
      },
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.ByteTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGMtPVk78Jks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b5c13c-acff-4a69-babb-3e4b223d9343"
      },
      "source": [
        "pair_batch = pairs[:5]\n",
        "print(pair_batch)\n",
        "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "print(pair_batch)\n",
        "print(target_variable)\n",
        "print(mask)\n",
        "print(max_target_len)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['there . ', 'where ? '], ['you have my word . a a gentleman ', 'you be sweet . '], ['hi . ', 'look like thing work out tonight huh ? '], ['have fun tonight ? ', 'ton '], ['do you listen to this crap ? ', 'what crap ? ']]\n",
            "[['you have my word . a a gentleman ', 'you be sweet . '], ['do you listen to this crap ? ', 'what crap ? '], ['have fun tonight ? ', 'ton '], ['there . ', 'where ? '], ['hi . ', 'look like thing work out tonight huh ? ']]\n",
            "tensor([[  43,   25,   18,   49,  101],\n",
            "        [1876,   36,    9,   91,    4],\n",
            "        [  56,   57,   12,  218,   40],\n",
            "        [  56,  604, 3700,   59,   36],\n",
            "        [  56,  171,   78, 1658,    8],\n",
            "        [   5,   46,   43,    4,   36],\n",
            "        [   2,   35, 5024,    5,    7],\n",
            "        [   0,    4,    5,    2,    5],\n",
            "        [   0,    5,    2,    0,    2],\n",
            "        [   0,    2,    0,    0,    0]])\n",
            "tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [0, 1, 1, 1, 1],\n",
            "        [0, 1, 1, 0, 1],\n",
            "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Arb62b3rYDX"
      },
      "source": [
        "# **Encoder**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sosjNS7NUUKz"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        \n",
        "        embedded = self.embedding(input_seq)\n",
        "        \n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        \n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        \n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        \n",
        "        return outputs, hidden\n",
        "    def init_hidden(self):\n",
        "        \n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SnxT22mBa6x"
      },
      "source": [
        "# **attention weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhlXIwPgUUK2"
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmbRfBUgq-dn"
      },
      "source": [
        "# **Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvriR9lUUK5"
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        \n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "       \n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "    \n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOP18l8UUK8"
      },
      "source": [
        "# Define Training Procedure\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMKwBNIkUUK9"
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q7ZQQO_q5Y9"
      },
      "source": [
        "# **Train Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUbyiWguUULA"
      },
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "  \n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    \n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    \n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    \n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    \n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            \n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            \n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            \n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            \n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    \n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02kHrXWgUULG"
      },
      "source": [
        "# **Training iterations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIVgVHbuUULH"
      },
      "source": [
        "def fun1(pairs):\n",
        "  global m\n",
        "  global s\n",
        "  #for p in pairs:\n",
        "  if m>=len(pairs):\n",
        "    m=0\n",
        "    s+=1\n",
        "  m+=1\n",
        "  #print(s,m-1,len(pairs),pairs[m-1])\n",
        "  return pairs[m-1]\n",
        "m=0\n",
        "s=1\n",
        "\n",
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "\n",
        "    training_batches = [batch2TrainData(voc, [fun1(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    print('number of epoch  {}'.format(s))\n",
        "#    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "#                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    \n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    losslist = []\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    \n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        \n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        \n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "            losslist.append(print_loss_avg)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
        "    return losslist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r1Kg8JYUULR"
      },
      "source": [
        "# **Evaluate my text**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2xV9PGnUULT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a6f6cf78-44e2-48b5-822b-bea532a8a2bb"
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "  \n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    \n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    \n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    \n",
        "    input_batch = input_batch.to(device)\n",
        "    #*******lengths = lengths.to(device)\n",
        "    \n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    \n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    \n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "'''\n",
        "Dep=0\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    global Dep\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('You :   ')\n",
        "\n",
        "            Dep+=fD2(input_sentence)\n",
        "            #vs = analyzer.polarity_scores(input_sentence)\n",
        "            mm=scaleD(input_sentence)\n",
        "            #print(\"Scale.{}\".format(mm))\n",
        "            \n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit' or input_sentence == 'bye' : break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot :   ', ' '.join(output_words))\n",
        "            \n",
        "            #print('Diagnos {}'.format(Dep))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")\n",
        "    if(Dep>5):\n",
        "      Rscale(mm)\n",
        "      print('Diagnos {}'.format(Dep))\n",
        "    else:\n",
        "      print(\" >>  your psychological state is great, you are fine. ^^ \")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDep=0\\ndef evaluateInput(encoder, decoder, searcher, voc):\\n    input_sentence = \\'\\'\\n    global Dep\\n    while(1):\\n        try:\\n            # Get input sentence\\n            input_sentence = input(\\'You :   \\')\\n\\n            Dep+=fD2(input_sentence)\\n            #vs = analyzer.polarity_scores(input_sentence)\\n            mm=scaleD(input_sentence)\\n            #print(\"Scale.{}\".format(mm))\\n            \\n            # Check if it is quit case\\n            if input_sentence == \\'q\\' or input_sentence == \\'quit\\' or input_sentence == \\'bye\\' : break\\n            # Normalize sentence\\n            input_sentence = normalizeString(input_sentence)\\n            # Evaluate sentence\\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\\n            # Format and print response sentence\\n            output_words[:] = [x for x in output_words if not (x == \\'EOS\\' or x == \\'PAD\\')]\\n            print(\\'Bot :   \\', \\' \\'.join(output_words))\\n            \\n            #print(\\'Diagnos {}\\'.format(Dep))\\n\\n        except KeyError:\\n            print(\"Error: Encountered unknown word.\")\\n    if(Dep>5):\\n      Rscale(mm)\\n      print(\\'Diagnos {}\\'.format(Dep))\\n    else:\\n      print(\" >>  your psychological state is great, you are fine. ^^ \")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS3BsMC4eh11"
      },
      "source": [
        "# **Beam Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSpOTEtEeXP2"
      },
      "source": [
        "class Sentence:\n",
        "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
        "        if(len(sentence_idxes) != len(sentence_scores)):\n",
        "            raise ValueError(\"length of indexes and scores should be the same\")\n",
        "        self.decoder_hidden = decoder_hidden\n",
        "        self.last_idx = last_idx\n",
        "        self.sentence_idxes =  sentence_idxes\n",
        "        self.sentence_scores = sentence_scores\n",
        "\n",
        "    def avgScore(self):\n",
        "        if len(self.sentence_scores) == 0:\n",
        "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
        "        # return mean of sentence_score\n",
        "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
        "\n",
        "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
        "        topv = torch.log(topv)\n",
        "        terminates, sentences = [], []\n",
        "        for i in range(beam_size):\n",
        "            if topi[0][i] == EOS_token:\n",
        "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
        "                                   self.avgScore())) \n",
        "                continue\n",
        "            idxes = self.sentence_idxes[:] \n",
        "            scores = self.sentence_scores[:] \n",
        "            idxes.append(topi[0][i])\n",
        "            scores.append(topv[0][i])\n",
        "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
        "        return terminates, sentences\n",
        "\n",
        "    def toWordScore(self, voc):\n",
        "        \n",
        "        words = []\n",
        "        for i in range(len(self.sentence_idxes)):\n",
        "            if self.sentence_idxes[i] == EOS_token:\n",
        "                words.append('<EOS>')\n",
        "            else:\n",
        "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
        "       \n",
        "        if self.sentence_idxes[-1] != EOS_token:\n",
        "            words.append('<EOS>')\n",
        "        return (words, self.avgScore())\n",
        "\n",
        "    def __repr__(self):\n",
        "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
        "        res += f\"and scores {self.sentence_scores}\"\n",
        "        return res\n",
        "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
        "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
        "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
        "    for i in range(max_length):\n",
        "        \n",
        "        for sentence in prev_top_sentences:\n",
        "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "\n",
        "            decoder_hidden = sentence.decoder_hidden\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(beam_size)\n",
        "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
        "            terminal_sentences.extend(term)\n",
        "            next_top_sentences.extend(top)\n",
        "           \n",
        "        \n",
        "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
        "        prev_top_sentences = next_top_sentences[:beam_size]\n",
        "        next_top_sentences = []\n",
        "        \n",
        "\n",
        "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
        "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    n = min(len(terminal_sentences), 15)\n",
        "    return terminal_sentences[:n]\n",
        "  \n",
        "class BeamSearchDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
        "        super(BeamSearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.voc = voc\n",
        "        self.beam_size = beam_size\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        \n",
        "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "        \n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
        "        \n",
        "        \n",
        "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
        "        return all_tokens, None\n",
        "\n",
        "    def __str__(self):\n",
        "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
        "        return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zaFMWYVUULW"
      },
      "source": [
        "# **Run Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFpsfSS2UULZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3b1401-f272-4837-bd7a-8224b249953f"
      },
      "source": [
        "model_name = 'try Final flask '\n",
        "attn_model = 'dot'\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "corpus_name=\"trian5\"\n",
        "\n",
        "hidden_size = 512\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 4\n",
        "dropout = 0.5\n",
        "batch_size = 256 \n",
        "loadFilename = None\n",
        "checkpoint_iter = 10000\n",
        "'''\n",
        "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "                            '{}_checkpoint.tar'.format(checkpoint_iter))'''\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    #checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUSGlN8wUULh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fbd8c7-185c-43eb-d750-19a5688c4325"
      },
      "source": [
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 10000\n",
        "print_every = 10\n",
        "save_every = 2000\n",
        "loadFilename = None\n",
        "checkpoint_iter = 10000\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "print(\"Starting Training!\")\n",
        "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training!\n",
            "number of epoch  58\n",
            "Initializing ...\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:28.)\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:175: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/IndexKernel.cpp:62.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10; Percent complete: 0.1%; Average loss: 7.7138\n",
            "Iteration: 20; Percent complete: 0.2%; Average loss: 5.2014\n",
            "Iteration: 30; Percent complete: 0.3%; Average loss: 4.4948\n",
            "Iteration: 40; Percent complete: 0.4%; Average loss: 4.4455\n",
            "Iteration: 50; Percent complete: 0.5%; Average loss: 4.3537\n",
            "Iteration: 60; Percent complete: 0.6%; Average loss: 4.3397\n",
            "Iteration: 70; Percent complete: 0.7%; Average loss: 4.1693\n",
            "Iteration: 80; Percent complete: 0.8%; Average loss: 4.2312\n",
            "Iteration: 90; Percent complete: 0.9%; Average loss: 4.1648\n",
            "Iteration: 100; Percent complete: 1.0%; Average loss: 4.1479\n",
            "Iteration: 110; Percent complete: 1.1%; Average loss: 4.0656\n",
            "Iteration: 120; Percent complete: 1.2%; Average loss: 3.9877\n",
            "Iteration: 130; Percent complete: 1.3%; Average loss: 3.7870\n",
            "Iteration: 140; Percent complete: 1.4%; Average loss: 3.7430\n",
            "Iteration: 150; Percent complete: 1.5%; Average loss: 3.6746\n",
            "Iteration: 160; Percent complete: 1.6%; Average loss: 3.6457\n",
            "Iteration: 170; Percent complete: 1.7%; Average loss: 3.6777\n",
            "Iteration: 180; Percent complete: 1.8%; Average loss: 3.6079\n",
            "Iteration: 190; Percent complete: 1.9%; Average loss: 3.5887\n",
            "Iteration: 200; Percent complete: 2.0%; Average loss: 3.5554\n",
            "Iteration: 210; Percent complete: 2.1%; Average loss: 3.5600\n",
            "Iteration: 220; Percent complete: 2.2%; Average loss: 3.5963\n",
            "Iteration: 230; Percent complete: 2.3%; Average loss: 3.5644\n",
            "Iteration: 240; Percent complete: 2.4%; Average loss: 3.5211\n",
            "Iteration: 250; Percent complete: 2.5%; Average loss: 3.4715\n",
            "Iteration: 260; Percent complete: 2.6%; Average loss: 3.4833\n",
            "Iteration: 270; Percent complete: 2.7%; Average loss: 3.5264\n",
            "Iteration: 280; Percent complete: 2.8%; Average loss: 3.4770\n",
            "Iteration: 290; Percent complete: 2.9%; Average loss: 3.4338\n",
            "Iteration: 300; Percent complete: 3.0%; Average loss: 3.4180\n",
            "Iteration: 310; Percent complete: 3.1%; Average loss: 3.3790\n",
            "Iteration: 320; Percent complete: 3.2%; Average loss: 3.5320\n",
            "Iteration: 330; Percent complete: 3.3%; Average loss: 3.4371\n",
            "Iteration: 340; Percent complete: 3.4%; Average loss: 3.4889\n",
            "Iteration: 350; Percent complete: 3.5%; Average loss: 3.4036\n",
            "Iteration: 360; Percent complete: 3.6%; Average loss: 3.3941\n",
            "Iteration: 370; Percent complete: 3.7%; Average loss: 3.4148\n",
            "Iteration: 380; Percent complete: 3.8%; Average loss: 3.3780\n",
            "Iteration: 390; Percent complete: 3.9%; Average loss: 3.4191\n",
            "Iteration: 400; Percent complete: 4.0%; Average loss: 3.3747\n",
            "Iteration: 410; Percent complete: 4.1%; Average loss: 3.3924\n",
            "Iteration: 420; Percent complete: 4.2%; Average loss: 3.3277\n",
            "Iteration: 430; Percent complete: 4.3%; Average loss: 3.3463\n",
            "Iteration: 440; Percent complete: 4.4%; Average loss: 3.3615\n",
            "Iteration: 450; Percent complete: 4.5%; Average loss: 3.3167\n",
            "Iteration: 460; Percent complete: 4.6%; Average loss: 3.2992\n",
            "Iteration: 470; Percent complete: 4.7%; Average loss: 3.3250\n",
            "Iteration: 480; Percent complete: 4.8%; Average loss: 3.2407\n",
            "Iteration: 490; Percent complete: 4.9%; Average loss: 3.3776\n",
            "Iteration: 500; Percent complete: 5.0%; Average loss: 3.3472\n",
            "Iteration: 510; Percent complete: 5.1%; Average loss: 3.3037\n",
            "Iteration: 520; Percent complete: 5.2%; Average loss: 3.3697\n",
            "Iteration: 530; Percent complete: 5.3%; Average loss: 3.2693\n",
            "Iteration: 540; Percent complete: 5.4%; Average loss: 3.3465\n",
            "Iteration: 550; Percent complete: 5.5%; Average loss: 3.2921\n",
            "Iteration: 560; Percent complete: 5.6%; Average loss: 3.2723\n",
            "Iteration: 570; Percent complete: 5.7%; Average loss: 3.3013\n",
            "Iteration: 580; Percent complete: 5.8%; Average loss: 3.3391\n",
            "Iteration: 590; Percent complete: 5.9%; Average loss: 3.1892\n",
            "Iteration: 600; Percent complete: 6.0%; Average loss: 3.3208\n",
            "Iteration: 610; Percent complete: 6.1%; Average loss: 3.2170\n",
            "Iteration: 620; Percent complete: 6.2%; Average loss: 3.2713\n",
            "Iteration: 630; Percent complete: 6.3%; Average loss: 3.2246\n",
            "Iteration: 640; Percent complete: 6.4%; Average loss: 3.2402\n",
            "Iteration: 650; Percent complete: 6.5%; Average loss: 3.1733\n",
            "Iteration: 660; Percent complete: 6.6%; Average loss: 3.2452\n",
            "Iteration: 670; Percent complete: 6.7%; Average loss: 3.2525\n",
            "Iteration: 680; Percent complete: 6.8%; Average loss: 3.2431\n",
            "Iteration: 690; Percent complete: 6.9%; Average loss: 3.2862\n",
            "Iteration: 700; Percent complete: 7.0%; Average loss: 3.2351\n",
            "Iteration: 710; Percent complete: 7.1%; Average loss: 3.2221\n",
            "Iteration: 720; Percent complete: 7.2%; Average loss: 3.2170\n",
            "Iteration: 730; Percent complete: 7.3%; Average loss: 3.2201\n",
            "Iteration: 740; Percent complete: 7.4%; Average loss: 3.2603\n",
            "Iteration: 750; Percent complete: 7.5%; Average loss: 3.2004\n",
            "Iteration: 760; Percent complete: 7.6%; Average loss: 3.1980\n",
            "Iteration: 770; Percent complete: 7.7%; Average loss: 3.1398\n",
            "Iteration: 780; Percent complete: 7.8%; Average loss: 3.1657\n",
            "Iteration: 790; Percent complete: 7.9%; Average loss: 3.2072\n",
            "Iteration: 800; Percent complete: 8.0%; Average loss: 3.1850\n",
            "Iteration: 810; Percent complete: 8.1%; Average loss: 3.1411\n",
            "Iteration: 820; Percent complete: 8.2%; Average loss: 3.1261\n",
            "Iteration: 830; Percent complete: 8.3%; Average loss: 3.0865\n",
            "Iteration: 840; Percent complete: 8.4%; Average loss: 3.2489\n",
            "Iteration: 850; Percent complete: 8.5%; Average loss: 3.1534\n",
            "Iteration: 860; Percent complete: 8.6%; Average loss: 3.2109\n",
            "Iteration: 870; Percent complete: 8.7%; Average loss: 3.1324\n",
            "Iteration: 880; Percent complete: 8.8%; Average loss: 3.1260\n",
            "Iteration: 890; Percent complete: 8.9%; Average loss: 3.1385\n",
            "Iteration: 900; Percent complete: 9.0%; Average loss: 3.1244\n",
            "Iteration: 910; Percent complete: 9.1%; Average loss: 3.1672\n",
            "Iteration: 920; Percent complete: 9.2%; Average loss: 3.1061\n",
            "Iteration: 930; Percent complete: 9.3%; Average loss: 3.1282\n",
            "Iteration: 940; Percent complete: 9.4%; Average loss: 3.0734\n",
            "Iteration: 950; Percent complete: 9.5%; Average loss: 3.0972\n",
            "Iteration: 960; Percent complete: 9.6%; Average loss: 3.1021\n",
            "Iteration: 970; Percent complete: 9.7%; Average loss: 3.0621\n",
            "Iteration: 980; Percent complete: 9.8%; Average loss: 3.0489\n",
            "Iteration: 990; Percent complete: 9.9%; Average loss: 3.0839\n",
            "Iteration: 1000; Percent complete: 10.0%; Average loss: 2.9799\n",
            "Iteration: 1010; Percent complete: 10.1%; Average loss: 3.1337\n",
            "Iteration: 1020; Percent complete: 10.2%; Average loss: 3.0866\n",
            "Iteration: 1030; Percent complete: 10.3%; Average loss: 3.0421\n",
            "Iteration: 1040; Percent complete: 10.4%; Average loss: 3.1038\n",
            "Iteration: 1050; Percent complete: 10.5%; Average loss: 3.0196\n",
            "Iteration: 1060; Percent complete: 10.6%; Average loss: 3.0860\n",
            "Iteration: 1070; Percent complete: 10.7%; Average loss: 3.0375\n",
            "Iteration: 1080; Percent complete: 10.8%; Average loss: 3.0200\n",
            "Iteration: 1090; Percent complete: 10.9%; Average loss: 3.0507\n",
            "Iteration: 1100; Percent complete: 11.0%; Average loss: 3.0838\n",
            "Iteration: 1110; Percent complete: 11.1%; Average loss: 2.9335\n",
            "Iteration: 1120; Percent complete: 11.2%; Average loss: 3.0956\n",
            "Iteration: 1130; Percent complete: 11.3%; Average loss: 2.9644\n",
            "Iteration: 1140; Percent complete: 11.4%; Average loss: 3.0210\n",
            "Iteration: 1150; Percent complete: 11.5%; Average loss: 2.9688\n",
            "Iteration: 1160; Percent complete: 11.6%; Average loss: 3.0014\n",
            "Iteration: 1170; Percent complete: 11.7%; Average loss: 2.9317\n",
            "Iteration: 1180; Percent complete: 11.8%; Average loss: 2.9985\n",
            "Iteration: 1190; Percent complete: 11.9%; Average loss: 2.9959\n",
            "Iteration: 1200; Percent complete: 12.0%; Average loss: 3.0010\n",
            "Iteration: 1210; Percent complete: 12.1%; Average loss: 3.0281\n",
            "Iteration: 1220; Percent complete: 12.2%; Average loss: 2.9758\n",
            "Iteration: 1230; Percent complete: 12.3%; Average loss: 2.9695\n",
            "Iteration: 1240; Percent complete: 12.4%; Average loss: 2.9746\n",
            "Iteration: 1250; Percent complete: 12.5%; Average loss: 2.9642\n",
            "Iteration: 1260; Percent complete: 12.6%; Average loss: 3.0118\n",
            "Iteration: 1270; Percent complete: 12.7%; Average loss: 2.9534\n",
            "Iteration: 1280; Percent complete: 12.8%; Average loss: 2.9640\n",
            "Iteration: 1290; Percent complete: 12.9%; Average loss: 2.8979\n",
            "Iteration: 1300; Percent complete: 13.0%; Average loss: 2.9321\n",
            "Iteration: 1310; Percent complete: 13.1%; Average loss: 2.9638\n",
            "Iteration: 1320; Percent complete: 13.2%; Average loss: 2.9504\n",
            "Iteration: 1330; Percent complete: 13.3%; Average loss: 2.9023\n",
            "Iteration: 1340; Percent complete: 13.4%; Average loss: 2.8852\n",
            "Iteration: 1350; Percent complete: 13.5%; Average loss: 2.8503\n",
            "Iteration: 1360; Percent complete: 13.6%; Average loss: 3.0222\n",
            "Iteration: 1370; Percent complete: 13.7%; Average loss: 2.9150\n",
            "Iteration: 1380; Percent complete: 13.8%; Average loss: 2.9721\n",
            "Iteration: 1390; Percent complete: 13.9%; Average loss: 2.8905\n",
            "Iteration: 1400; Percent complete: 14.0%; Average loss: 2.8844\n",
            "Iteration: 1410; Percent complete: 14.1%; Average loss: 2.9230\n",
            "Iteration: 1420; Percent complete: 14.2%; Average loss: 2.8995\n",
            "Iteration: 1430; Percent complete: 14.3%; Average loss: 2.9501\n",
            "Iteration: 1440; Percent complete: 14.4%; Average loss: 2.8745\n",
            "Iteration: 1450; Percent complete: 14.5%; Average loss: 2.9236\n",
            "Iteration: 1460; Percent complete: 14.6%; Average loss: 2.8455\n",
            "Iteration: 1470; Percent complete: 14.7%; Average loss: 2.8904\n",
            "Iteration: 1480; Percent complete: 14.8%; Average loss: 2.8883\n",
            "Iteration: 1490; Percent complete: 14.9%; Average loss: 2.8554\n",
            "Iteration: 1500; Percent complete: 15.0%; Average loss: 2.8529\n",
            "Iteration: 1510; Percent complete: 15.1%; Average loss: 2.8827\n",
            "Iteration: 1520; Percent complete: 15.2%; Average loss: 2.7698\n",
            "Iteration: 1530; Percent complete: 15.3%; Average loss: 2.9286\n",
            "Iteration: 1540; Percent complete: 15.4%; Average loss: 2.8814\n",
            "Iteration: 1550; Percent complete: 15.5%; Average loss: 2.8386\n",
            "Iteration: 1560; Percent complete: 15.6%; Average loss: 2.8930\n",
            "Iteration: 1570; Percent complete: 15.7%; Average loss: 2.8282\n",
            "Iteration: 1580; Percent complete: 15.8%; Average loss: 2.8797\n",
            "Iteration: 1590; Percent complete: 15.9%; Average loss: 2.8506\n",
            "Iteration: 1600; Percent complete: 16.0%; Average loss: 2.8249\n",
            "Iteration: 1610; Percent complete: 16.1%; Average loss: 2.8530\n",
            "Iteration: 1620; Percent complete: 16.2%; Average loss: 2.8945\n",
            "Iteration: 1630; Percent complete: 16.3%; Average loss: 2.7429\n",
            "Iteration: 1640; Percent complete: 16.4%; Average loss: 2.9013\n",
            "Iteration: 1650; Percent complete: 16.5%; Average loss: 2.7627\n",
            "Iteration: 1660; Percent complete: 16.6%; Average loss: 2.8409\n",
            "Iteration: 1670; Percent complete: 16.7%; Average loss: 2.7871\n",
            "Iteration: 1680; Percent complete: 16.8%; Average loss: 2.8129\n",
            "Iteration: 1690; Percent complete: 16.9%; Average loss: 2.7601\n",
            "Iteration: 1700; Percent complete: 17.0%; Average loss: 2.8020\n",
            "Iteration: 1710; Percent complete: 17.1%; Average loss: 2.8098\n",
            "Iteration: 1720; Percent complete: 17.2%; Average loss: 2.8183\n",
            "Iteration: 1730; Percent complete: 17.3%; Average loss: 2.8343\n",
            "Iteration: 1740; Percent complete: 17.4%; Average loss: 2.7874\n",
            "Iteration: 1750; Percent complete: 17.5%; Average loss: 2.7845\n",
            "Iteration: 1760; Percent complete: 17.6%; Average loss: 2.7964\n",
            "Iteration: 1770; Percent complete: 17.7%; Average loss: 2.7952\n",
            "Iteration: 1780; Percent complete: 17.8%; Average loss: 2.8270\n",
            "Iteration: 1790; Percent complete: 17.9%; Average loss: 2.7748\n",
            "Iteration: 1800; Percent complete: 18.0%; Average loss: 2.7736\n",
            "Iteration: 1810; Percent complete: 18.1%; Average loss: 2.7358\n",
            "Iteration: 1820; Percent complete: 18.2%; Average loss: 2.7509\n",
            "Iteration: 1830; Percent complete: 18.3%; Average loss: 2.7787\n",
            "Iteration: 1840; Percent complete: 18.4%; Average loss: 2.7821\n",
            "Iteration: 1850; Percent complete: 18.5%; Average loss: 2.7350\n",
            "Iteration: 1860; Percent complete: 18.6%; Average loss: 2.7184\n",
            "Iteration: 1870; Percent complete: 18.7%; Average loss: 2.6823\n",
            "Iteration: 1880; Percent complete: 18.8%; Average loss: 2.8334\n",
            "Iteration: 1890; Percent complete: 18.9%; Average loss: 2.7467\n",
            "Iteration: 1900; Percent complete: 19.0%; Average loss: 2.7868\n",
            "Iteration: 1910; Percent complete: 19.1%; Average loss: 2.7161\n",
            "Iteration: 1920; Percent complete: 19.2%; Average loss: 2.7009\n",
            "Iteration: 1930; Percent complete: 19.3%; Average loss: 2.7569\n",
            "Iteration: 1940; Percent complete: 19.4%; Average loss: 2.7319\n",
            "Iteration: 1950; Percent complete: 19.5%; Average loss: 2.7802\n",
            "Iteration: 1960; Percent complete: 19.6%; Average loss: 2.7059\n",
            "Iteration: 1970; Percent complete: 19.7%; Average loss: 2.7413\n",
            "Iteration: 1980; Percent complete: 19.8%; Average loss: 2.6693\n",
            "Iteration: 1990; Percent complete: 19.9%; Average loss: 2.7189\n",
            "Iteration: 2000; Percent complete: 20.0%; Average loss: 2.7159\n",
            "Iteration: 2010; Percent complete: 20.1%; Average loss: 2.6975\n",
            "Iteration: 2020; Percent complete: 20.2%; Average loss: 2.6819\n",
            "Iteration: 2030; Percent complete: 20.3%; Average loss: 2.7233\n",
            "Iteration: 2040; Percent complete: 20.4%; Average loss: 2.6017\n",
            "Iteration: 2050; Percent complete: 20.5%; Average loss: 2.7435\n",
            "Iteration: 2060; Percent complete: 20.6%; Average loss: 2.7100\n",
            "Iteration: 2070; Percent complete: 20.7%; Average loss: 2.6574\n",
            "Iteration: 2080; Percent complete: 20.8%; Average loss: 2.7155\n",
            "Iteration: 2090; Percent complete: 20.9%; Average loss: 2.6626\n",
            "Iteration: 2100; Percent complete: 21.0%; Average loss: 2.6983\n",
            "Iteration: 2110; Percent complete: 21.1%; Average loss: 2.6873\n",
            "Iteration: 2120; Percent complete: 21.2%; Average loss: 2.6614\n",
            "Iteration: 2130; Percent complete: 21.3%; Average loss: 2.6857\n",
            "Iteration: 2140; Percent complete: 21.4%; Average loss: 2.7133\n",
            "Iteration: 2150; Percent complete: 21.5%; Average loss: 2.5787\n",
            "Iteration: 2160; Percent complete: 21.6%; Average loss: 2.7247\n",
            "Iteration: 2170; Percent complete: 21.7%; Average loss: 2.5914\n",
            "Iteration: 2180; Percent complete: 21.8%; Average loss: 2.6729\n",
            "Iteration: 2190; Percent complete: 21.9%; Average loss: 2.6299\n",
            "Iteration: 2200; Percent complete: 22.0%; Average loss: 2.6448\n",
            "Iteration: 2210; Percent complete: 22.1%; Average loss: 2.5998\n",
            "Iteration: 2220; Percent complete: 22.2%; Average loss: 2.6330\n",
            "Iteration: 2230; Percent complete: 22.3%; Average loss: 2.6440\n",
            "Iteration: 2240; Percent complete: 22.4%; Average loss: 2.6370\n",
            "Iteration: 2250; Percent complete: 22.5%; Average loss: 2.6545\n",
            "Iteration: 2260; Percent complete: 22.6%; Average loss: 2.6107\n",
            "Iteration: 2270; Percent complete: 22.7%; Average loss: 2.6209\n",
            "Iteration: 2280; Percent complete: 22.8%; Average loss: 2.6351\n",
            "Iteration: 2290; Percent complete: 22.9%; Average loss: 2.6344\n",
            "Iteration: 2300; Percent complete: 23.0%; Average loss: 2.6577\n",
            "Iteration: 2310; Percent complete: 23.1%; Average loss: 2.6087\n",
            "Iteration: 2320; Percent complete: 23.2%; Average loss: 2.5946\n",
            "Iteration: 2330; Percent complete: 23.3%; Average loss: 2.5666\n",
            "Iteration: 2340; Percent complete: 23.4%; Average loss: 2.5870\n",
            "Iteration: 2350; Percent complete: 23.5%; Average loss: 2.6026\n",
            "Iteration: 2360; Percent complete: 23.6%; Average loss: 2.6146\n",
            "Iteration: 2370; Percent complete: 23.7%; Average loss: 2.5695\n",
            "Iteration: 2380; Percent complete: 23.8%; Average loss: 2.5549\n",
            "Iteration: 2390; Percent complete: 23.9%; Average loss: 2.5127\n",
            "Iteration: 2400; Percent complete: 24.0%; Average loss: 2.6550\n",
            "Iteration: 2410; Percent complete: 24.1%; Average loss: 2.5800\n",
            "Iteration: 2420; Percent complete: 24.2%; Average loss: 2.6135\n",
            "Iteration: 2430; Percent complete: 24.3%; Average loss: 2.5413\n",
            "Iteration: 2440; Percent complete: 24.4%; Average loss: 2.5312\n",
            "Iteration: 2450; Percent complete: 24.5%; Average loss: 2.5898\n",
            "Iteration: 2460; Percent complete: 24.6%; Average loss: 2.5668\n",
            "Iteration: 2470; Percent complete: 24.7%; Average loss: 2.6107\n",
            "Iteration: 2480; Percent complete: 24.8%; Average loss: 2.5351\n",
            "Iteration: 2490; Percent complete: 24.9%; Average loss: 2.5628\n",
            "Iteration: 2500; Percent complete: 25.0%; Average loss: 2.5038\n",
            "Iteration: 2510; Percent complete: 25.1%; Average loss: 2.5463\n",
            "Iteration: 2520; Percent complete: 25.2%; Average loss: 2.5489\n",
            "Iteration: 2530; Percent complete: 25.3%; Average loss: 2.5325\n",
            "Iteration: 2540; Percent complete: 25.4%; Average loss: 2.5105\n",
            "Iteration: 2550; Percent complete: 25.5%; Average loss: 2.5576\n",
            "Iteration: 2560; Percent complete: 25.6%; Average loss: 2.4503\n",
            "Iteration: 2570; Percent complete: 25.7%; Average loss: 2.5699\n",
            "Iteration: 2580; Percent complete: 25.8%; Average loss: 2.5316\n",
            "Iteration: 2590; Percent complete: 25.9%; Average loss: 2.4849\n",
            "Iteration: 2600; Percent complete: 26.0%; Average loss: 2.5337\n",
            "Iteration: 2610; Percent complete: 26.1%; Average loss: 2.4966\n",
            "Iteration: 2620; Percent complete: 26.2%; Average loss: 2.5268\n",
            "Iteration: 2630; Percent complete: 26.3%; Average loss: 2.5316\n",
            "Iteration: 2640; Percent complete: 26.4%; Average loss: 2.4908\n",
            "Iteration: 2650; Percent complete: 26.5%; Average loss: 2.5024\n",
            "Iteration: 2660; Percent complete: 26.6%; Average loss: 2.5378\n",
            "Iteration: 2670; Percent complete: 26.7%; Average loss: 2.4113\n",
            "Iteration: 2680; Percent complete: 26.8%; Average loss: 2.5613\n",
            "Iteration: 2690; Percent complete: 26.9%; Average loss: 2.4267\n",
            "Iteration: 2700; Percent complete: 27.0%; Average loss: 2.5088\n",
            "Iteration: 2710; Percent complete: 27.1%; Average loss: 2.4570\n",
            "Iteration: 2720; Percent complete: 27.2%; Average loss: 2.4777\n",
            "Iteration: 2730; Percent complete: 27.3%; Average loss: 2.4376\n",
            "Iteration: 2740; Percent complete: 27.4%; Average loss: 2.4650\n",
            "Iteration: 2750; Percent complete: 27.5%; Average loss: 2.4664\n",
            "Iteration: 2760; Percent complete: 27.6%; Average loss: 2.4663\n",
            "Iteration: 2770; Percent complete: 27.7%; Average loss: 2.4693\n",
            "Iteration: 2780; Percent complete: 27.8%; Average loss: 2.4365\n",
            "Iteration: 2790; Percent complete: 27.9%; Average loss: 2.4488\n",
            "Iteration: 2800; Percent complete: 28.0%; Average loss: 2.4685\n",
            "Iteration: 2810; Percent complete: 28.1%; Average loss: 2.4731\n",
            "Iteration: 2820; Percent complete: 28.2%; Average loss: 2.4848\n",
            "Iteration: 2830; Percent complete: 28.3%; Average loss: 2.4251\n",
            "Iteration: 2840; Percent complete: 28.4%; Average loss: 2.4241\n",
            "Iteration: 2850; Percent complete: 28.5%; Average loss: 2.4080\n",
            "Iteration: 2860; Percent complete: 28.6%; Average loss: 2.4265\n",
            "Iteration: 2870; Percent complete: 28.7%; Average loss: 2.4386\n",
            "Iteration: 2880; Percent complete: 28.8%; Average loss: 2.4433\n",
            "Iteration: 2890; Percent complete: 28.9%; Average loss: 2.3997\n",
            "Iteration: 2900; Percent complete: 29.0%; Average loss: 2.4026\n",
            "Iteration: 2910; Percent complete: 29.1%; Average loss: 2.3555\n",
            "Iteration: 2920; Percent complete: 29.2%; Average loss: 2.4770\n",
            "Iteration: 2930; Percent complete: 29.3%; Average loss: 2.4116\n",
            "Iteration: 2940; Percent complete: 29.4%; Average loss: 2.4391\n",
            "Iteration: 2950; Percent complete: 29.5%; Average loss: 2.3650\n",
            "Iteration: 2960; Percent complete: 29.6%; Average loss: 2.3547\n",
            "Iteration: 2970; Percent complete: 29.7%; Average loss: 2.4126\n",
            "Iteration: 2980; Percent complete: 29.8%; Average loss: 2.4204\n",
            "Iteration: 2990; Percent complete: 29.9%; Average loss: 2.4429\n",
            "Iteration: 3000; Percent complete: 30.0%; Average loss: 2.3683\n",
            "Iteration: 3010; Percent complete: 30.1%; Average loss: 2.3853\n",
            "Iteration: 3020; Percent complete: 30.2%; Average loss: 2.3303\n",
            "Iteration: 3030; Percent complete: 30.3%; Average loss: 2.3843\n",
            "Iteration: 3040; Percent complete: 30.4%; Average loss: 2.3862\n",
            "Iteration: 3050; Percent complete: 30.5%; Average loss: 2.3777\n",
            "Iteration: 3060; Percent complete: 30.6%; Average loss: 2.3453\n",
            "Iteration: 3070; Percent complete: 30.7%; Average loss: 2.3920\n",
            "Iteration: 3080; Percent complete: 30.8%; Average loss: 2.2919\n",
            "Iteration: 3090; Percent complete: 30.9%; Average loss: 2.3941\n",
            "Iteration: 3100; Percent complete: 31.0%; Average loss: 2.3679\n",
            "Iteration: 3110; Percent complete: 31.1%; Average loss: 2.3302\n",
            "Iteration: 3120; Percent complete: 31.2%; Average loss: 2.3489\n",
            "Iteration: 3130; Percent complete: 31.3%; Average loss: 2.3430\n",
            "Iteration: 3140; Percent complete: 31.4%; Average loss: 2.3522\n",
            "Iteration: 3150; Percent complete: 31.5%; Average loss: 2.3768\n",
            "Iteration: 3160; Percent complete: 31.6%; Average loss: 2.3372\n",
            "Iteration: 3170; Percent complete: 31.7%; Average loss: 2.3505\n",
            "Iteration: 3180; Percent complete: 31.8%; Average loss: 2.3604\n",
            "Iteration: 3190; Percent complete: 31.9%; Average loss: 2.2433\n",
            "Iteration: 3200; Percent complete: 32.0%; Average loss: 2.3934\n",
            "Iteration: 3210; Percent complete: 32.1%; Average loss: 2.2833\n",
            "Iteration: 3220; Percent complete: 32.2%; Average loss: 2.3536\n",
            "Iteration: 3230; Percent complete: 32.3%; Average loss: 2.3021\n",
            "Iteration: 3240; Percent complete: 32.4%; Average loss: 2.3102\n",
            "Iteration: 3250; Percent complete: 32.5%; Average loss: 2.2817\n",
            "Iteration: 3260; Percent complete: 32.6%; Average loss: 2.2918\n",
            "Iteration: 3270; Percent complete: 32.7%; Average loss: 2.3139\n",
            "Iteration: 3280; Percent complete: 32.8%; Average loss: 2.3092\n",
            "Iteration: 3290; Percent complete: 32.9%; Average loss: 2.2935\n",
            "Iteration: 3300; Percent complete: 33.0%; Average loss: 2.2675\n",
            "Iteration: 3310; Percent complete: 33.1%; Average loss: 2.2842\n",
            "Iteration: 3320; Percent complete: 33.2%; Average loss: 2.2978\n",
            "Iteration: 3330; Percent complete: 33.3%; Average loss: 2.3094\n",
            "Iteration: 3340; Percent complete: 33.4%; Average loss: 2.3278\n",
            "Iteration: 3350; Percent complete: 33.5%; Average loss: 2.2622\n",
            "Iteration: 3360; Percent complete: 33.6%; Average loss: 2.2552\n",
            "Iteration: 3370; Percent complete: 33.7%; Average loss: 2.2570\n",
            "Iteration: 3380; Percent complete: 33.8%; Average loss: 2.2751\n",
            "Iteration: 3390; Percent complete: 33.9%; Average loss: 2.2907\n",
            "Iteration: 3400; Percent complete: 34.0%; Average loss: 2.2941\n",
            "Iteration: 3410; Percent complete: 34.1%; Average loss: 2.2369\n",
            "Iteration: 3420; Percent complete: 34.2%; Average loss: 2.2559\n",
            "Iteration: 3430; Percent complete: 34.3%; Average loss: 2.1956\n",
            "Iteration: 3440; Percent complete: 34.4%; Average loss: 2.2928\n",
            "Iteration: 3450; Percent complete: 34.5%; Average loss: 2.2557\n",
            "Iteration: 3460; Percent complete: 34.6%; Average loss: 2.2674\n",
            "Iteration: 3470; Percent complete: 34.7%; Average loss: 2.2044\n",
            "Iteration: 3480; Percent complete: 34.8%; Average loss: 2.1829\n",
            "Iteration: 3490; Percent complete: 34.9%; Average loss: 2.2483\n",
            "Iteration: 3500; Percent complete: 35.0%; Average loss: 2.2630\n",
            "Iteration: 3510; Percent complete: 35.1%; Average loss: 2.2762\n",
            "Iteration: 3520; Percent complete: 35.2%; Average loss: 2.2180\n",
            "Iteration: 3530; Percent complete: 35.3%; Average loss: 2.2162\n",
            "Iteration: 3540; Percent complete: 35.4%; Average loss: 2.1682\n",
            "Iteration: 3550; Percent complete: 35.5%; Average loss: 2.2338\n",
            "Iteration: 3560; Percent complete: 35.6%; Average loss: 2.2478\n",
            "Iteration: 3570; Percent complete: 35.7%; Average loss: 2.2244\n",
            "Iteration: 3580; Percent complete: 35.8%; Average loss: 2.1889\n",
            "Iteration: 3590; Percent complete: 35.9%; Average loss: 2.2413\n",
            "Iteration: 3600; Percent complete: 36.0%; Average loss: 2.1392\n",
            "Iteration: 3610; Percent complete: 36.1%; Average loss: 2.2176\n",
            "Iteration: 3620; Percent complete: 36.2%; Average loss: 2.1996\n",
            "Iteration: 3630; Percent complete: 36.3%; Average loss: 2.1746\n",
            "Iteration: 3640; Percent complete: 36.4%; Average loss: 2.1643\n",
            "Iteration: 3650; Percent complete: 36.5%; Average loss: 2.1707\n",
            "Iteration: 3660; Percent complete: 36.6%; Average loss: 2.1710\n",
            "Iteration: 3670; Percent complete: 36.7%; Average loss: 2.2183\n",
            "Iteration: 3680; Percent complete: 36.8%; Average loss: 2.1759\n",
            "Iteration: 3690; Percent complete: 36.9%; Average loss: 2.1893\n",
            "Iteration: 3700; Percent complete: 37.0%; Average loss: 2.1837\n",
            "Iteration: 3710; Percent complete: 37.1%; Average loss: 2.0870\n",
            "Iteration: 3720; Percent complete: 37.2%; Average loss: 2.2386\n",
            "Iteration: 3730; Percent complete: 37.3%; Average loss: 2.1430\n",
            "Iteration: 3740; Percent complete: 37.4%; Average loss: 2.2016\n",
            "Iteration: 3750; Percent complete: 37.5%; Average loss: 2.1546\n",
            "Iteration: 3760; Percent complete: 37.6%; Average loss: 2.1531\n",
            "Iteration: 3770; Percent complete: 37.7%; Average loss: 2.1390\n",
            "Iteration: 3780; Percent complete: 37.8%; Average loss: 2.1273\n",
            "Iteration: 3790; Percent complete: 37.9%; Average loss: 2.1528\n",
            "Iteration: 3800; Percent complete: 38.0%; Average loss: 2.1511\n",
            "Iteration: 3810; Percent complete: 38.1%; Average loss: 2.1171\n",
            "Iteration: 3820; Percent complete: 38.2%; Average loss: 2.1086\n",
            "Iteration: 3830; Percent complete: 38.3%; Average loss: 2.1223\n",
            "Iteration: 3840; Percent complete: 38.4%; Average loss: 2.1415\n",
            "Iteration: 3850; Percent complete: 38.5%; Average loss: 2.1497\n",
            "Iteration: 3860; Percent complete: 38.6%; Average loss: 2.1708\n",
            "Iteration: 3870; Percent complete: 38.7%; Average loss: 2.0943\n",
            "Iteration: 3880; Percent complete: 38.8%; Average loss: 2.0857\n",
            "Iteration: 3890; Percent complete: 38.9%; Average loss: 2.0989\n",
            "Iteration: 3900; Percent complete: 39.0%; Average loss: 2.1328\n",
            "Iteration: 3910; Percent complete: 39.1%; Average loss: 2.1390\n",
            "Iteration: 3920; Percent complete: 39.2%; Average loss: 2.1243\n",
            "Iteration: 3930; Percent complete: 39.3%; Average loss: 2.0930\n",
            "Iteration: 3940; Percent complete: 39.4%; Average loss: 2.1061\n",
            "Iteration: 3950; Percent complete: 39.5%; Average loss: 2.0410\n",
            "Iteration: 3960; Percent complete: 39.6%; Average loss: 2.1215\n",
            "Iteration: 3970; Percent complete: 39.7%; Average loss: 2.0999\n",
            "Iteration: 3980; Percent complete: 39.8%; Average loss: 2.1034\n",
            "Iteration: 3990; Percent complete: 39.9%; Average loss: 2.0293\n",
            "Iteration: 4000; Percent complete: 40.0%; Average loss: 2.0311\n",
            "Iteration: 4010; Percent complete: 40.1%; Average loss: 2.0777\n",
            "Iteration: 4020; Percent complete: 40.2%; Average loss: 2.1146\n",
            "Iteration: 4030; Percent complete: 40.3%; Average loss: 2.1128\n",
            "Iteration: 4040; Percent complete: 40.4%; Average loss: 2.0653\n",
            "Iteration: 4050; Percent complete: 40.5%; Average loss: 2.0415\n",
            "Iteration: 4060; Percent complete: 40.6%; Average loss: 2.0178\n",
            "Iteration: 4070; Percent complete: 40.7%; Average loss: 2.0801\n",
            "Iteration: 4080; Percent complete: 40.8%; Average loss: 2.1002\n",
            "Iteration: 4090; Percent complete: 40.9%; Average loss: 2.0683\n",
            "Iteration: 4100; Percent complete: 41.0%; Average loss: 2.0311\n",
            "Iteration: 4110; Percent complete: 41.1%; Average loss: 2.0860\n",
            "Iteration: 4120; Percent complete: 41.2%; Average loss: 1.9904\n",
            "Iteration: 4130; Percent complete: 41.3%; Average loss: 2.0495\n",
            "Iteration: 4140; Percent complete: 41.4%; Average loss: 2.0358\n",
            "Iteration: 4150; Percent complete: 41.5%; Average loss: 2.0162\n",
            "Iteration: 4160; Percent complete: 41.6%; Average loss: 1.9941\n",
            "Iteration: 4170; Percent complete: 41.7%; Average loss: 2.0196\n",
            "Iteration: 4180; Percent complete: 41.8%; Average loss: 2.0066\n",
            "Iteration: 4190; Percent complete: 41.9%; Average loss: 2.0534\n",
            "Iteration: 4200; Percent complete: 42.0%; Average loss: 2.0283\n",
            "Iteration: 4210; Percent complete: 42.1%; Average loss: 2.0325\n",
            "Iteration: 4220; Percent complete: 42.2%; Average loss: 2.0013\n",
            "Iteration: 4230; Percent complete: 42.3%; Average loss: 1.9330\n",
            "Iteration: 4240; Percent complete: 42.4%; Average loss: 2.0710\n",
            "Iteration: 4250; Percent complete: 42.5%; Average loss: 2.0019\n",
            "Iteration: 4260; Percent complete: 42.6%; Average loss: 2.0368\n",
            "Iteration: 4270; Percent complete: 42.7%; Average loss: 2.0002\n",
            "Iteration: 4280; Percent complete: 42.8%; Average loss: 1.9970\n",
            "Iteration: 4290; Percent complete: 42.9%; Average loss: 1.9754\n",
            "Iteration: 4300; Percent complete: 43.0%; Average loss: 1.9736\n",
            "Iteration: 4310; Percent complete: 43.1%; Average loss: 1.9843\n",
            "Iteration: 4320; Percent complete: 43.2%; Average loss: 1.9843\n",
            "Iteration: 4330; Percent complete: 43.3%; Average loss: 1.9459\n",
            "Iteration: 4340; Percent complete: 43.4%; Average loss: 1.9513\n",
            "Iteration: 4350; Percent complete: 43.5%; Average loss: 1.9564\n",
            "Iteration: 4360; Percent complete: 43.6%; Average loss: 1.9850\n",
            "Iteration: 4370; Percent complete: 43.7%; Average loss: 1.9886\n",
            "Iteration: 4380; Percent complete: 43.8%; Average loss: 2.0070\n",
            "Iteration: 4390; Percent complete: 43.9%; Average loss: 1.9331\n",
            "Iteration: 4400; Percent complete: 44.0%; Average loss: 1.9170\n",
            "Iteration: 4410; Percent complete: 44.1%; Average loss: 1.9400\n",
            "Iteration: 4420; Percent complete: 44.2%; Average loss: 1.9723\n",
            "Iteration: 4430; Percent complete: 44.3%; Average loss: 1.9722\n",
            "Iteration: 4440; Percent complete: 44.4%; Average loss: 1.9661\n",
            "Iteration: 4450; Percent complete: 44.5%; Average loss: 1.9537\n",
            "Iteration: 4460; Percent complete: 44.6%; Average loss: 1.9504\n",
            "Iteration: 4470; Percent complete: 44.7%; Average loss: 1.8917\n",
            "Iteration: 4480; Percent complete: 44.8%; Average loss: 1.9487\n",
            "Iteration: 4490; Percent complete: 44.9%; Average loss: 1.9254\n",
            "Iteration: 4500; Percent complete: 45.0%; Average loss: 1.9347\n",
            "Iteration: 4510; Percent complete: 45.1%; Average loss: 1.8753\n",
            "Iteration: 4520; Percent complete: 45.2%; Average loss: 1.8593\n",
            "Iteration: 4530; Percent complete: 45.3%; Average loss: 1.9218\n",
            "Iteration: 4540; Percent complete: 45.4%; Average loss: 1.9511\n",
            "Iteration: 4550; Percent complete: 45.5%; Average loss: 1.9640\n",
            "Iteration: 4560; Percent complete: 45.6%; Average loss: 1.9177\n",
            "Iteration: 4570; Percent complete: 45.7%; Average loss: 1.8744\n",
            "Iteration: 4580; Percent complete: 45.8%; Average loss: 1.8491\n",
            "Iteration: 4590; Percent complete: 45.9%; Average loss: 1.9125\n",
            "Iteration: 4600; Percent complete: 46.0%; Average loss: 1.9432\n",
            "Iteration: 4610; Percent complete: 46.1%; Average loss: 1.9196\n",
            "Iteration: 4620; Percent complete: 46.2%; Average loss: 1.8709\n",
            "Iteration: 4630; Percent complete: 46.3%; Average loss: 1.9289\n",
            "Iteration: 4640; Percent complete: 46.4%; Average loss: 1.8447\n",
            "Iteration: 4650; Percent complete: 46.5%; Average loss: 1.9000\n",
            "Iteration: 4660; Percent complete: 46.6%; Average loss: 1.8626\n",
            "Iteration: 4670; Percent complete: 46.7%; Average loss: 1.8601\n",
            "Iteration: 4680; Percent complete: 46.8%; Average loss: 1.8238\n",
            "Iteration: 4690; Percent complete: 46.9%; Average loss: 1.8590\n",
            "Iteration: 4700; Percent complete: 47.0%; Average loss: 1.8253\n",
            "Iteration: 4710; Percent complete: 47.1%; Average loss: 1.9018\n",
            "Iteration: 4720; Percent complete: 47.2%; Average loss: 1.8747\n",
            "Iteration: 4730; Percent complete: 47.3%; Average loss: 1.8907\n",
            "Iteration: 4740; Percent complete: 47.4%; Average loss: 1.8387\n",
            "Iteration: 4750; Percent complete: 47.5%; Average loss: 1.7701\n",
            "Iteration: 4760; Percent complete: 47.6%; Average loss: 1.8873\n",
            "Iteration: 4770; Percent complete: 47.7%; Average loss: 1.8434\n",
            "Iteration: 4780; Percent complete: 47.8%; Average loss: 1.8805\n",
            "Iteration: 4790; Percent complete: 47.9%; Average loss: 1.8543\n",
            "Iteration: 4800; Percent complete: 48.0%; Average loss: 1.8402\n",
            "Iteration: 4810; Percent complete: 48.1%; Average loss: 1.8285\n",
            "Iteration: 4820; Percent complete: 48.2%; Average loss: 1.8246\n",
            "Iteration: 4830; Percent complete: 48.3%; Average loss: 1.8312\n",
            "Iteration: 4840; Percent complete: 48.4%; Average loss: 1.8116\n",
            "Iteration: 4850; Percent complete: 48.5%; Average loss: 1.7709\n",
            "Iteration: 4860; Percent complete: 48.6%; Average loss: 1.7851\n",
            "Iteration: 4870; Percent complete: 48.7%; Average loss: 1.7846\n",
            "Iteration: 4880; Percent complete: 48.8%; Average loss: 1.8177\n",
            "Iteration: 4890; Percent complete: 48.9%; Average loss: 1.8323\n",
            "Iteration: 4900; Percent complete: 49.0%; Average loss: 1.8581\n",
            "Iteration: 4910; Percent complete: 49.1%; Average loss: 1.7882\n",
            "Iteration: 4920; Percent complete: 49.2%; Average loss: 1.7521\n",
            "Iteration: 4930; Percent complete: 49.3%; Average loss: 1.7759\n",
            "Iteration: 4940; Percent complete: 49.4%; Average loss: 1.7960\n",
            "Iteration: 4950; Percent complete: 49.5%; Average loss: 1.8148\n",
            "Iteration: 4960; Percent complete: 49.6%; Average loss: 1.8115\n",
            "Iteration: 4970; Percent complete: 49.7%; Average loss: 1.7942\n",
            "Iteration: 4980; Percent complete: 49.8%; Average loss: 1.8057\n",
            "Iteration: 4990; Percent complete: 49.9%; Average loss: 1.7464\n",
            "Iteration: 5000; Percent complete: 50.0%; Average loss: 1.7917\n",
            "Iteration: 5010; Percent complete: 50.1%; Average loss: 1.7625\n",
            "Iteration: 5020; Percent complete: 50.2%; Average loss: 1.7754\n",
            "Iteration: 5030; Percent complete: 50.3%; Average loss: 1.7171\n",
            "Iteration: 5040; Percent complete: 50.4%; Average loss: 1.7011\n",
            "Iteration: 5050; Percent complete: 50.5%; Average loss: 1.7588\n",
            "Iteration: 5060; Percent complete: 50.6%; Average loss: 1.7902\n",
            "Iteration: 5070; Percent complete: 50.7%; Average loss: 1.8087\n",
            "Iteration: 5080; Percent complete: 50.8%; Average loss: 1.7654\n",
            "Iteration: 5090; Percent complete: 50.9%; Average loss: 1.7144\n",
            "Iteration: 5100; Percent complete: 51.0%; Average loss: 1.6983\n",
            "Iteration: 5110; Percent complete: 51.1%; Average loss: 1.7407\n",
            "Iteration: 5120; Percent complete: 51.2%; Average loss: 1.7821\n",
            "Iteration: 5130; Percent complete: 51.3%; Average loss: 1.7581\n",
            "Iteration: 5140; Percent complete: 51.4%; Average loss: 1.7274\n",
            "Iteration: 5150; Percent complete: 51.5%; Average loss: 1.7830\n",
            "Iteration: 5160; Percent complete: 51.6%; Average loss: 1.7011\n",
            "Iteration: 5170; Percent complete: 51.7%; Average loss: 1.7397\n",
            "Iteration: 5180; Percent complete: 51.8%; Average loss: 1.7079\n",
            "Iteration: 5190; Percent complete: 51.9%; Average loss: 1.7092\n",
            "Iteration: 5200; Percent complete: 52.0%; Average loss: 1.6605\n",
            "Iteration: 5210; Percent complete: 52.1%; Average loss: 1.7025\n",
            "Iteration: 5220; Percent complete: 52.2%; Average loss: 1.6667\n",
            "Iteration: 5230; Percent complete: 52.3%; Average loss: 1.7498\n",
            "Iteration: 5240; Percent complete: 52.4%; Average loss: 1.7205\n",
            "Iteration: 5250; Percent complete: 52.5%; Average loss: 1.7419\n",
            "Iteration: 5260; Percent complete: 52.6%; Average loss: 1.6884\n",
            "Iteration: 5270; Percent complete: 52.7%; Average loss: 1.6384\n",
            "Iteration: 5280; Percent complete: 52.8%; Average loss: 1.7267\n",
            "Iteration: 5290; Percent complete: 52.9%; Average loss: 1.6951\n",
            "Iteration: 5300; Percent complete: 53.0%; Average loss: 1.7146\n",
            "Iteration: 5310; Percent complete: 53.1%; Average loss: 1.6984\n",
            "Iteration: 5320; Percent complete: 53.2%; Average loss: 1.7044\n",
            "Iteration: 5330; Percent complete: 53.3%; Average loss: 1.6797\n",
            "Iteration: 5340; Percent complete: 53.4%; Average loss: 1.6702\n",
            "Iteration: 5350; Percent complete: 53.5%; Average loss: 1.6866\n",
            "Iteration: 5360; Percent complete: 53.6%; Average loss: 1.6635\n",
            "Iteration: 5370; Percent complete: 53.7%; Average loss: 1.6166\n",
            "Iteration: 5380; Percent complete: 53.8%; Average loss: 1.6460\n",
            "Iteration: 5390; Percent complete: 53.9%; Average loss: 1.6207\n",
            "Iteration: 5400; Percent complete: 54.0%; Average loss: 1.6732\n",
            "Iteration: 5410; Percent complete: 54.1%; Average loss: 1.6808\n",
            "Iteration: 5420; Percent complete: 54.2%; Average loss: 1.7072\n",
            "Iteration: 5430; Percent complete: 54.3%; Average loss: 1.6494\n",
            "Iteration: 5440; Percent complete: 54.4%; Average loss: 1.6208\n",
            "Iteration: 5450; Percent complete: 54.5%; Average loss: 1.6354\n",
            "Iteration: 5460; Percent complete: 54.6%; Average loss: 1.6478\n",
            "Iteration: 5470; Percent complete: 54.7%; Average loss: 1.6610\n",
            "Iteration: 5480; Percent complete: 54.8%; Average loss: 1.6528\n",
            "Iteration: 5490; Percent complete: 54.9%; Average loss: 1.6518\n",
            "Iteration: 5500; Percent complete: 55.0%; Average loss: 1.6672\n",
            "Iteration: 5510; Percent complete: 55.1%; Average loss: 1.6075\n",
            "Iteration: 5520; Percent complete: 55.2%; Average loss: 1.6429\n",
            "Iteration: 5530; Percent complete: 55.3%; Average loss: 1.6135\n",
            "Iteration: 5540; Percent complete: 55.4%; Average loss: 1.6292\n",
            "Iteration: 5550; Percent complete: 55.5%; Average loss: 1.5667\n",
            "Iteration: 5560; Percent complete: 55.6%; Average loss: 1.5542\n",
            "Iteration: 5570; Percent complete: 55.7%; Average loss: 1.6030\n",
            "Iteration: 5580; Percent complete: 55.8%; Average loss: 1.6524\n",
            "Iteration: 5590; Percent complete: 55.9%; Average loss: 1.6414\n",
            "Iteration: 5600; Percent complete: 56.0%; Average loss: 1.6243\n",
            "Iteration: 5610; Percent complete: 56.1%; Average loss: 1.5788\n",
            "Iteration: 5620; Percent complete: 56.2%; Average loss: 1.5554\n",
            "Iteration: 5630; Percent complete: 56.3%; Average loss: 1.6019\n",
            "Iteration: 5640; Percent complete: 56.4%; Average loss: 1.6243\n",
            "Iteration: 5650; Percent complete: 56.5%; Average loss: 1.6114\n",
            "Iteration: 5660; Percent complete: 56.6%; Average loss: 1.5740\n",
            "Iteration: 5670; Percent complete: 56.7%; Average loss: 1.6310\n",
            "Iteration: 5680; Percent complete: 56.8%; Average loss: 1.5875\n",
            "Iteration: 5690; Percent complete: 56.9%; Average loss: 1.5922\n",
            "Iteration: 5700; Percent complete: 57.0%; Average loss: 1.5604\n",
            "Iteration: 5710; Percent complete: 57.1%; Average loss: 1.5727\n",
            "Iteration: 5720; Percent complete: 57.2%; Average loss: 1.5274\n",
            "Iteration: 5730; Percent complete: 57.3%; Average loss: 1.5571\n",
            "Iteration: 5740; Percent complete: 57.4%; Average loss: 1.5198\n",
            "Iteration: 5750; Percent complete: 57.5%; Average loss: 1.5909\n",
            "Iteration: 5760; Percent complete: 57.6%; Average loss: 1.5801\n",
            "Iteration: 5770; Percent complete: 57.7%; Average loss: 1.5962\n",
            "Iteration: 5780; Percent complete: 57.8%; Average loss: 1.5455\n",
            "Iteration: 5790; Percent complete: 57.9%; Average loss: 1.5092\n",
            "Iteration: 5800; Percent complete: 58.0%; Average loss: 1.5753\n",
            "Iteration: 5810; Percent complete: 58.1%; Average loss: 1.5699\n",
            "Iteration: 5820; Percent complete: 58.2%; Average loss: 1.5735\n",
            "Iteration: 5830; Percent complete: 58.3%; Average loss: 1.5546\n",
            "Iteration: 5840; Percent complete: 58.4%; Average loss: 1.5700\n",
            "Iteration: 5850; Percent complete: 58.5%; Average loss: 1.5416\n",
            "Iteration: 5860; Percent complete: 58.6%; Average loss: 1.5317\n",
            "Iteration: 5870; Percent complete: 58.7%; Average loss: 1.5370\n",
            "Iteration: 5880; Percent complete: 58.8%; Average loss: 1.5135\n",
            "Iteration: 5890; Percent complete: 58.9%; Average loss: 1.4685\n",
            "Iteration: 5900; Percent complete: 59.0%; Average loss: 1.5066\n",
            "Iteration: 5910; Percent complete: 59.1%; Average loss: 1.4783\n",
            "Iteration: 5920; Percent complete: 59.2%; Average loss: 1.5165\n",
            "Iteration: 5930; Percent complete: 59.3%; Average loss: 1.5307\n",
            "Iteration: 5940; Percent complete: 59.4%; Average loss: 1.5558\n",
            "Iteration: 5950; Percent complete: 59.5%; Average loss: 1.4938\n",
            "Iteration: 5960; Percent complete: 59.6%; Average loss: 1.4833\n",
            "Iteration: 5970; Percent complete: 59.7%; Average loss: 1.4764\n",
            "Iteration: 5980; Percent complete: 59.8%; Average loss: 1.5178\n",
            "Iteration: 5990; Percent complete: 59.9%; Average loss: 1.5353\n",
            "Iteration: 6000; Percent complete: 60.0%; Average loss: 1.5203\n",
            "Iteration: 6010; Percent complete: 60.1%; Average loss: 1.5093\n",
            "Iteration: 6020; Percent complete: 60.2%; Average loss: 1.5260\n",
            "Iteration: 6030; Percent complete: 60.3%; Average loss: 1.4745\n",
            "Iteration: 6040; Percent complete: 60.4%; Average loss: 1.5015\n",
            "Iteration: 6050; Percent complete: 60.5%; Average loss: 1.4770\n",
            "Iteration: 6060; Percent complete: 60.6%; Average loss: 1.4760\n",
            "Iteration: 6070; Percent complete: 60.7%; Average loss: 1.4138\n",
            "Iteration: 6080; Percent complete: 60.8%; Average loss: 1.4247\n",
            "Iteration: 6090; Percent complete: 60.9%; Average loss: 1.4649\n",
            "Iteration: 6100; Percent complete: 61.0%; Average loss: 1.4986\n",
            "Iteration: 6110; Percent complete: 61.1%; Average loss: 1.4921\n",
            "Iteration: 6120; Percent complete: 61.2%; Average loss: 1.4788\n",
            "Iteration: 6130; Percent complete: 61.3%; Average loss: 1.4298\n",
            "Iteration: 6140; Percent complete: 61.4%; Average loss: 1.4239\n",
            "Iteration: 6150; Percent complete: 61.5%; Average loss: 1.4670\n",
            "Iteration: 6160; Percent complete: 61.6%; Average loss: 1.5024\n",
            "Iteration: 6170; Percent complete: 61.7%; Average loss: 1.4798\n",
            "Iteration: 6180; Percent complete: 61.8%; Average loss: 1.4488\n",
            "Iteration: 6190; Percent complete: 61.9%; Average loss: 1.4899\n",
            "Iteration: 6200; Percent complete: 62.0%; Average loss: 1.4556\n",
            "Iteration: 6210; Percent complete: 62.1%; Average loss: 1.4566\n",
            "Iteration: 6220; Percent complete: 62.2%; Average loss: 1.4183\n",
            "Iteration: 6230; Percent complete: 62.3%; Average loss: 1.4288\n",
            "Iteration: 6240; Percent complete: 62.4%; Average loss: 1.3783\n",
            "Iteration: 6250; Percent complete: 62.5%; Average loss: 1.4146\n",
            "Iteration: 6260; Percent complete: 62.6%; Average loss: 1.3698\n",
            "Iteration: 6270; Percent complete: 62.7%; Average loss: 1.4559\n",
            "Iteration: 6280; Percent complete: 62.8%; Average loss: 1.4448\n",
            "Iteration: 6290; Percent complete: 62.9%; Average loss: 1.4481\n",
            "Iteration: 6300; Percent complete: 63.0%; Average loss: 1.3860\n",
            "Iteration: 6310; Percent complete: 63.1%; Average loss: 1.3688\n",
            "Iteration: 6320; Percent complete: 63.2%; Average loss: 1.4302\n",
            "Iteration: 6330; Percent complete: 63.3%; Average loss: 1.4326\n",
            "Iteration: 6340; Percent complete: 63.4%; Average loss: 1.4407\n",
            "Iteration: 6350; Percent complete: 63.5%; Average loss: 1.4277\n",
            "Iteration: 6360; Percent complete: 63.6%; Average loss: 1.4356\n",
            "Iteration: 6370; Percent complete: 63.7%; Average loss: 1.4214\n",
            "Iteration: 6380; Percent complete: 63.8%; Average loss: 1.4084\n",
            "Iteration: 6390; Percent complete: 63.9%; Average loss: 1.4132\n",
            "Iteration: 6400; Percent complete: 64.0%; Average loss: 1.3737\n",
            "Iteration: 6410; Percent complete: 64.1%; Average loss: 1.3430\n",
            "Iteration: 6420; Percent complete: 64.2%; Average loss: 1.3902\n",
            "Iteration: 6430; Percent complete: 64.3%; Average loss: 1.3504\n",
            "Iteration: 6440; Percent complete: 64.4%; Average loss: 1.3774\n",
            "Iteration: 6450; Percent complete: 64.5%; Average loss: 1.4134\n",
            "Iteration: 6460; Percent complete: 64.6%; Average loss: 1.4124\n",
            "Iteration: 6470; Percent complete: 64.7%; Average loss: 1.3551\n",
            "Iteration: 6480; Percent complete: 64.8%; Average loss: 1.3483\n",
            "Iteration: 6490; Percent complete: 64.9%; Average loss: 1.3391\n",
            "Iteration: 6500; Percent complete: 65.0%; Average loss: 1.3740\n",
            "Iteration: 6510; Percent complete: 65.1%; Average loss: 1.4005\n",
            "Iteration: 6520; Percent complete: 65.2%; Average loss: 1.3901\n",
            "Iteration: 6530; Percent complete: 65.3%; Average loss: 1.3903\n",
            "Iteration: 6540; Percent complete: 65.4%; Average loss: 1.4015\n",
            "Iteration: 6550; Percent complete: 65.5%; Average loss: 1.3569\n",
            "Iteration: 6560; Percent complete: 65.6%; Average loss: 1.3749\n",
            "Iteration: 6570; Percent complete: 65.7%; Average loss: 1.3467\n",
            "Iteration: 6580; Percent complete: 65.8%; Average loss: 1.3580\n",
            "Iteration: 6590; Percent complete: 65.9%; Average loss: 1.2868\n",
            "Iteration: 6600; Percent complete: 66.0%; Average loss: 1.2969\n",
            "Iteration: 6610; Percent complete: 66.1%; Average loss: 1.3322\n",
            "Iteration: 6620; Percent complete: 66.2%; Average loss: 1.3710\n",
            "Iteration: 6630; Percent complete: 66.3%; Average loss: 1.3563\n",
            "Iteration: 6640; Percent complete: 66.4%; Average loss: 1.3373\n",
            "Iteration: 6650; Percent complete: 66.5%; Average loss: 1.2894\n",
            "Iteration: 6660; Percent complete: 66.6%; Average loss: 1.2868\n",
            "Iteration: 6670; Percent complete: 66.7%; Average loss: 1.3301\n",
            "Iteration: 6680; Percent complete: 66.8%; Average loss: 1.3710\n",
            "Iteration: 6690; Percent complete: 66.9%; Average loss: 1.3698\n",
            "Iteration: 6700; Percent complete: 67.0%; Average loss: 1.3321\n",
            "Iteration: 6710; Percent complete: 67.1%; Average loss: 1.3610\n",
            "Iteration: 6720; Percent complete: 67.2%; Average loss: 1.3417\n",
            "Iteration: 6730; Percent complete: 67.3%; Average loss: 1.3233\n",
            "Iteration: 6740; Percent complete: 67.4%; Average loss: 1.3048\n",
            "Iteration: 6750; Percent complete: 67.5%; Average loss: 1.3052\n",
            "Iteration: 6760; Percent complete: 67.6%; Average loss: 1.2534\n",
            "Iteration: 6770; Percent complete: 67.7%; Average loss: 1.2949\n",
            "Iteration: 6780; Percent complete: 67.8%; Average loss: 1.2430\n",
            "Iteration: 6790; Percent complete: 67.9%; Average loss: 1.3263\n",
            "Iteration: 6800; Percent complete: 68.0%; Average loss: 1.3168\n",
            "Iteration: 6810; Percent complete: 68.1%; Average loss: 1.3054\n",
            "Iteration: 6820; Percent complete: 68.2%; Average loss: 1.2597\n",
            "Iteration: 6830; Percent complete: 68.3%; Average loss: 1.2378\n",
            "Iteration: 6840; Percent complete: 68.4%; Average loss: 1.2851\n",
            "Iteration: 6850; Percent complete: 68.5%; Average loss: 1.3014\n",
            "Iteration: 6860; Percent complete: 68.6%; Average loss: 1.3054\n",
            "Iteration: 6870; Percent complete: 68.7%; Average loss: 1.3014\n",
            "Iteration: 6880; Percent complete: 68.8%; Average loss: 1.3143\n",
            "Iteration: 6890; Percent complete: 68.9%; Average loss: 1.3065\n",
            "Iteration: 6900; Percent complete: 69.0%; Average loss: 1.2840\n",
            "Iteration: 6910; Percent complete: 69.1%; Average loss: 1.3000\n",
            "Iteration: 6920; Percent complete: 69.2%; Average loss: 1.2362\n",
            "Iteration: 6930; Percent complete: 69.3%; Average loss: 1.2223\n",
            "Iteration: 6940; Percent complete: 69.4%; Average loss: 1.2587\n",
            "Iteration: 6950; Percent complete: 69.5%; Average loss: 1.2166\n",
            "Iteration: 6960; Percent complete: 69.6%; Average loss: 1.2531\n",
            "Iteration: 6970; Percent complete: 69.7%; Average loss: 1.2852\n",
            "Iteration: 6980; Percent complete: 69.8%; Average loss: 1.2749\n",
            "Iteration: 6990; Percent complete: 69.9%; Average loss: 1.2283\n",
            "Iteration: 7000; Percent complete: 70.0%; Average loss: 1.2129\n",
            "Iteration: 7010; Percent complete: 70.1%; Average loss: 1.2108\n",
            "Iteration: 7020; Percent complete: 70.2%; Average loss: 1.2403\n",
            "Iteration: 7030; Percent complete: 70.3%; Average loss: 1.2688\n",
            "Iteration: 7040; Percent complete: 70.4%; Average loss: 1.2585\n",
            "Iteration: 7050; Percent complete: 70.5%; Average loss: 1.2558\n",
            "Iteration: 7060; Percent complete: 70.6%; Average loss: 1.2823\n",
            "Iteration: 7070; Percent complete: 70.7%; Average loss: 1.2492\n",
            "Iteration: 7080; Percent complete: 70.8%; Average loss: 1.2379\n",
            "Iteration: 7090; Percent complete: 70.9%; Average loss: 1.2193\n",
            "Iteration: 7100; Percent complete: 71.0%; Average loss: 1.2369\n",
            "Iteration: 7110; Percent complete: 71.1%; Average loss: 1.1553\n",
            "Iteration: 7120; Percent complete: 71.2%; Average loss: 1.1722\n",
            "Iteration: 7130; Percent complete: 71.3%; Average loss: 1.1928\n",
            "Iteration: 7140; Percent complete: 71.4%; Average loss: 1.2515\n",
            "Iteration: 7150; Percent complete: 71.5%; Average loss: 1.2225\n",
            "Iteration: 7160; Percent complete: 71.6%; Average loss: 1.2095\n",
            "Iteration: 7170; Percent complete: 71.7%; Average loss: 1.1637\n",
            "Iteration: 7180; Percent complete: 71.8%; Average loss: 1.1747\n",
            "Iteration: 7190; Percent complete: 71.9%; Average loss: 1.1893\n",
            "Iteration: 7200; Percent complete: 72.0%; Average loss: 1.2208\n",
            "Iteration: 7210; Percent complete: 72.1%; Average loss: 1.2437\n",
            "Iteration: 7220; Percent complete: 72.2%; Average loss: 1.1973\n",
            "Iteration: 7230; Percent complete: 72.3%; Average loss: 1.2277\n",
            "Iteration: 7240; Percent complete: 72.4%; Average loss: 1.2252\n",
            "Iteration: 7250; Percent complete: 72.5%; Average loss: 1.2062\n",
            "Iteration: 7260; Percent complete: 72.6%; Average loss: 1.1851\n",
            "Iteration: 7270; Percent complete: 72.7%; Average loss: 1.1750\n",
            "Iteration: 7280; Percent complete: 72.8%; Average loss: 1.1370\n",
            "Iteration: 7290; Percent complete: 72.9%; Average loss: 1.1722\n",
            "Iteration: 7300; Percent complete: 73.0%; Average loss: 1.1231\n",
            "Iteration: 7310; Percent complete: 73.1%; Average loss: 1.2043\n",
            "Iteration: 7320; Percent complete: 73.2%; Average loss: 1.1938\n",
            "Iteration: 7330; Percent complete: 73.3%; Average loss: 1.1889\n",
            "Iteration: 7340; Percent complete: 73.4%; Average loss: 1.1237\n",
            "Iteration: 7350; Percent complete: 73.5%; Average loss: 1.1139\n",
            "Iteration: 7360; Percent complete: 73.6%; Average loss: 1.1623\n",
            "Iteration: 7370; Percent complete: 73.7%; Average loss: 1.1684\n",
            "Iteration: 7380; Percent complete: 73.8%; Average loss: 1.1724\n",
            "Iteration: 7390; Percent complete: 73.9%; Average loss: 1.1833\n",
            "Iteration: 7400; Percent complete: 74.0%; Average loss: 1.1957\n",
            "Iteration: 7410; Percent complete: 74.1%; Average loss: 1.1964\n",
            "Iteration: 7420; Percent complete: 74.2%; Average loss: 1.1707\n",
            "Iteration: 7430; Percent complete: 74.3%; Average loss: 1.1728\n",
            "Iteration: 7440; Percent complete: 74.4%; Average loss: 1.1278\n",
            "Iteration: 7450; Percent complete: 74.5%; Average loss: 1.1223\n",
            "Iteration: 7460; Percent complete: 74.6%; Average loss: 1.1224\n",
            "Iteration: 7470; Percent complete: 74.7%; Average loss: 1.1084\n",
            "Iteration: 7480; Percent complete: 74.8%; Average loss: 1.1226\n",
            "Iteration: 7490; Percent complete: 74.9%; Average loss: 1.1537\n",
            "Iteration: 7500; Percent complete: 75.0%; Average loss: 1.1538\n",
            "Iteration: 7510; Percent complete: 75.1%; Average loss: 1.0995\n",
            "Iteration: 7520; Percent complete: 75.2%; Average loss: 1.0833\n",
            "Iteration: 7530; Percent complete: 75.3%; Average loss: 1.0981\n",
            "Iteration: 7540; Percent complete: 75.4%; Average loss: 1.1139\n",
            "Iteration: 7550; Percent complete: 75.5%; Average loss: 1.1425\n",
            "Iteration: 7560; Percent complete: 75.6%; Average loss: 1.1451\n",
            "Iteration: 7570; Percent complete: 75.7%; Average loss: 1.1518\n",
            "Iteration: 7580; Percent complete: 75.8%; Average loss: 1.1608\n",
            "Iteration: 7590; Percent complete: 75.9%; Average loss: 1.1394\n",
            "Iteration: 7600; Percent complete: 76.0%; Average loss: 1.1169\n",
            "Iteration: 7610; Percent complete: 76.1%; Average loss: 1.1010\n",
            "Iteration: 7620; Percent complete: 76.2%; Average loss: 1.1122\n",
            "Iteration: 7630; Percent complete: 76.3%; Average loss: 1.0379\n",
            "Iteration: 7640; Percent complete: 76.4%; Average loss: 1.0746\n",
            "Iteration: 7650; Percent complete: 76.5%; Average loss: 1.0776\n",
            "Iteration: 7660; Percent complete: 76.6%; Average loss: 1.1312\n",
            "Iteration: 7670; Percent complete: 76.7%; Average loss: 1.1065\n",
            "Iteration: 7680; Percent complete: 76.8%; Average loss: 1.0941\n",
            "Iteration: 7690; Percent complete: 76.9%; Average loss: 1.0403\n",
            "Iteration: 7700; Percent complete: 77.0%; Average loss: 1.0539\n",
            "Iteration: 7710; Percent complete: 77.1%; Average loss: 1.0695\n",
            "Iteration: 7720; Percent complete: 77.2%; Average loss: 1.1078\n",
            "Iteration: 7730; Percent complete: 77.3%; Average loss: 1.1165\n",
            "Iteration: 7740; Percent complete: 77.4%; Average loss: 1.0944\n",
            "Iteration: 7750; Percent complete: 77.5%; Average loss: 1.1101\n",
            "Iteration: 7760; Percent complete: 77.6%; Average loss: 1.1153\n",
            "Iteration: 7770; Percent complete: 77.7%; Average loss: 1.0984\n",
            "Iteration: 7780; Percent complete: 77.8%; Average loss: 1.0757\n",
            "Iteration: 7790; Percent complete: 77.9%; Average loss: 1.0638\n",
            "Iteration: 7800; Percent complete: 78.0%; Average loss: 1.0183\n",
            "Iteration: 7810; Percent complete: 78.1%; Average loss: 1.0550\n",
            "Iteration: 7820; Percent complete: 78.2%; Average loss: 1.0216\n",
            "Iteration: 7830; Percent complete: 78.3%; Average loss: 1.0841\n",
            "Iteration: 7840; Percent complete: 78.4%; Average loss: 1.0807\n",
            "Iteration: 7850; Percent complete: 78.5%; Average loss: 1.0729\n",
            "Iteration: 7860; Percent complete: 78.6%; Average loss: 1.0111\n",
            "Iteration: 7870; Percent complete: 78.7%; Average loss: 0.9999\n",
            "Iteration: 7880; Percent complete: 78.8%; Average loss: 1.0501\n",
            "Iteration: 7890; Percent complete: 78.9%; Average loss: 1.0602\n",
            "Iteration: 7900; Percent complete: 79.0%; Average loss: 1.0527\n",
            "Iteration: 7910; Percent complete: 79.1%; Average loss: 1.0738\n",
            "Iteration: 7920; Percent complete: 79.2%; Average loss: 1.0715\n",
            "Iteration: 7930; Percent complete: 79.3%; Average loss: 1.0858\n",
            "Iteration: 7940; Percent complete: 79.4%; Average loss: 1.0514\n",
            "Iteration: 7950; Percent complete: 79.5%; Average loss: 1.0727\n",
            "Iteration: 7960; Percent complete: 79.6%; Average loss: 1.0139\n",
            "Iteration: 7970; Percent complete: 79.7%; Average loss: 1.0181\n",
            "Iteration: 7980; Percent complete: 79.8%; Average loss: 1.0093\n",
            "Iteration: 7990; Percent complete: 79.9%; Average loss: 0.9875\n",
            "Iteration: 8000; Percent complete: 80.0%; Average loss: 1.0128\n",
            "Iteration: 8010; Percent complete: 80.1%; Average loss: 1.0535\n",
            "Iteration: 8020; Percent complete: 80.2%; Average loss: 1.0422\n",
            "Iteration: 8030; Percent complete: 80.3%; Average loss: 0.9952\n",
            "Iteration: 8040; Percent complete: 80.4%; Average loss: 0.9909\n",
            "Iteration: 8050; Percent complete: 80.5%; Average loss: 0.9859\n",
            "Iteration: 8060; Percent complete: 80.6%; Average loss: 1.0023\n",
            "Iteration: 8070; Percent complete: 80.7%; Average loss: 1.0353\n",
            "Iteration: 8080; Percent complete: 80.8%; Average loss: 1.0398\n",
            "Iteration: 8090; Percent complete: 80.9%; Average loss: 1.0373\n",
            "Iteration: 8100; Percent complete: 81.0%; Average loss: 1.0454\n",
            "Iteration: 8110; Percent complete: 81.1%; Average loss: 1.0397\n",
            "Iteration: 8120; Percent complete: 81.2%; Average loss: 1.0204\n",
            "Iteration: 8130; Percent complete: 81.3%; Average loss: 0.9989\n",
            "Iteration: 8140; Percent complete: 81.4%; Average loss: 1.0193\n",
            "Iteration: 8150; Percent complete: 81.5%; Average loss: 0.9434\n",
            "Iteration: 8160; Percent complete: 81.6%; Average loss: 0.9743\n",
            "Iteration: 8170; Percent complete: 81.7%; Average loss: 0.9746\n",
            "Iteration: 8180; Percent complete: 81.8%; Average loss: 1.0183\n",
            "Iteration: 8190; Percent complete: 81.9%; Average loss: 0.9999\n",
            "Iteration: 8200; Percent complete: 82.0%; Average loss: 0.9817\n",
            "Iteration: 8210; Percent complete: 82.1%; Average loss: 0.9308\n",
            "Iteration: 8220; Percent complete: 82.2%; Average loss: 0.9506\n",
            "Iteration: 8230; Percent complete: 82.3%; Average loss: 0.9587\n",
            "Iteration: 8240; Percent complete: 82.4%; Average loss: 0.9982\n",
            "Iteration: 8250; Percent complete: 82.5%; Average loss: 1.0048\n",
            "Iteration: 8260; Percent complete: 82.6%; Average loss: 0.9967\n",
            "Iteration: 8270; Percent complete: 82.7%; Average loss: 0.9921\n",
            "Iteration: 8280; Percent complete: 82.8%; Average loss: 1.0213\n",
            "Iteration: 8290; Percent complete: 82.9%; Average loss: 1.0013\n",
            "Iteration: 8300; Percent complete: 83.0%; Average loss: 0.9900\n",
            "Iteration: 8310; Percent complete: 83.1%; Average loss: 0.9692\n",
            "Iteration: 8320; Percent complete: 83.2%; Average loss: 0.9141\n",
            "Iteration: 8330; Percent complete: 83.3%; Average loss: 0.9581\n",
            "Iteration: 8340; Percent complete: 83.4%; Average loss: 0.9181\n",
            "Iteration: 8350; Percent complete: 83.5%; Average loss: 0.9861\n",
            "Iteration: 8360; Percent complete: 83.6%; Average loss: 0.9697\n",
            "Iteration: 8370; Percent complete: 83.7%; Average loss: 0.9678\n",
            "Iteration: 8380; Percent complete: 83.8%; Average loss: 0.9050\n",
            "Iteration: 8390; Percent complete: 83.9%; Average loss: 0.9058\n",
            "Iteration: 8400; Percent complete: 84.0%; Average loss: 0.9418\n",
            "Iteration: 8410; Percent complete: 84.1%; Average loss: 0.9568\n",
            "Iteration: 8420; Percent complete: 84.2%; Average loss: 0.9466\n",
            "Iteration: 8430; Percent complete: 84.3%; Average loss: 0.9706\n",
            "Iteration: 8440; Percent complete: 84.4%; Average loss: 0.9759\n",
            "Iteration: 8450; Percent complete: 84.5%; Average loss: 0.9792\n",
            "Iteration: 8460; Percent complete: 84.6%; Average loss: 0.9588\n",
            "Iteration: 8470; Percent complete: 84.7%; Average loss: 0.9661\n",
            "Iteration: 8480; Percent complete: 84.8%; Average loss: 0.9246\n",
            "Iteration: 8490; Percent complete: 84.9%; Average loss: 0.9131\n",
            "Iteration: 8500; Percent complete: 85.0%; Average loss: 0.9130\n",
            "Iteration: 8510; Percent complete: 85.1%; Average loss: 0.8971\n",
            "Iteration: 8520; Percent complete: 85.2%; Average loss: 0.9248\n",
            "Iteration: 8530; Percent complete: 85.3%; Average loss: 0.9483\n",
            "Iteration: 8540; Percent complete: 85.4%; Average loss: 0.9433\n",
            "Iteration: 8550; Percent complete: 85.5%; Average loss: 0.8864\n",
            "Iteration: 8560; Percent complete: 85.6%; Average loss: 0.8889\n",
            "Iteration: 8570; Percent complete: 85.7%; Average loss: 0.8899\n",
            "Iteration: 8580; Percent complete: 85.8%; Average loss: 0.8908\n",
            "Iteration: 8590; Percent complete: 85.9%; Average loss: 0.9259\n",
            "Iteration: 8600; Percent complete: 86.0%; Average loss: 0.9267\n",
            "Iteration: 8610; Percent complete: 86.1%; Average loss: 0.9399\n",
            "Iteration: 8620; Percent complete: 86.2%; Average loss: 0.9490\n",
            "Iteration: 8630; Percent complete: 86.3%; Average loss: 0.9444\n",
            "Iteration: 8640; Percent complete: 86.4%; Average loss: 0.9197\n",
            "Iteration: 8650; Percent complete: 86.5%; Average loss: 0.9036\n",
            "Iteration: 8660; Percent complete: 86.6%; Average loss: 0.9185\n",
            "Iteration: 8670; Percent complete: 86.7%; Average loss: 0.8542\n",
            "Iteration: 8680; Percent complete: 86.8%; Average loss: 0.8824\n",
            "Iteration: 8690; Percent complete: 86.9%; Average loss: 0.8829\n",
            "Iteration: 8700; Percent complete: 87.0%; Average loss: 0.9225\n",
            "Iteration: 8710; Percent complete: 87.1%; Average loss: 0.9123\n",
            "Iteration: 8720; Percent complete: 87.2%; Average loss: 0.8822\n",
            "Iteration: 8730; Percent complete: 87.3%; Average loss: 0.8353\n",
            "Iteration: 8740; Percent complete: 87.4%; Average loss: 0.8550\n",
            "Iteration: 8750; Percent complete: 87.5%; Average loss: 0.8632\n",
            "Iteration: 8760; Percent complete: 87.6%; Average loss: 0.8896\n",
            "Iteration: 8770; Percent complete: 87.7%; Average loss: 0.8929\n",
            "Iteration: 8780; Percent complete: 87.8%; Average loss: 0.9091\n",
            "Iteration: 8790; Percent complete: 87.9%; Average loss: 0.8893\n",
            "Iteration: 8800; Percent complete: 88.0%; Average loss: 0.9252\n",
            "Iteration: 8810; Percent complete: 88.1%; Average loss: 0.9013\n",
            "Iteration: 8820; Percent complete: 88.2%; Average loss: 0.8916\n",
            "Iteration: 8830; Percent complete: 88.3%; Average loss: 0.8750\n",
            "Iteration: 8840; Percent complete: 88.4%; Average loss: 0.8299\n",
            "Iteration: 8850; Percent complete: 88.5%; Average loss: 0.8701\n",
            "Iteration: 8860; Percent complete: 88.6%; Average loss: 0.8363\n",
            "Iteration: 8870; Percent complete: 88.7%; Average loss: 0.8901\n",
            "Iteration: 8880; Percent complete: 88.8%; Average loss: 0.8795\n",
            "Iteration: 8890; Percent complete: 88.9%; Average loss: 0.8823\n",
            "Iteration: 8900; Percent complete: 89.0%; Average loss: 0.8141\n",
            "Iteration: 8910; Percent complete: 89.1%; Average loss: 0.8190\n",
            "Iteration: 8920; Percent complete: 89.2%; Average loss: 0.8513\n",
            "Iteration: 8930; Percent complete: 89.3%; Average loss: 0.8550\n",
            "Iteration: 8940; Percent complete: 89.4%; Average loss: 0.8460\n",
            "Iteration: 8950; Percent complete: 89.5%; Average loss: 0.8706\n",
            "Iteration: 8960; Percent complete: 89.6%; Average loss: 0.8892\n",
            "Iteration: 8970; Percent complete: 89.7%; Average loss: 0.8866\n",
            "Iteration: 8980; Percent complete: 89.8%; Average loss: 0.8573\n",
            "Iteration: 8990; Percent complete: 89.9%; Average loss: 0.8757\n",
            "Iteration: 9000; Percent complete: 90.0%; Average loss: 0.8418\n",
            "Iteration: 9010; Percent complete: 90.1%; Average loss: 0.8236\n",
            "Iteration: 9020; Percent complete: 90.2%; Average loss: 0.8291\n",
            "Iteration: 9030; Percent complete: 90.3%; Average loss: 0.8040\n",
            "Iteration: 9040; Percent complete: 90.4%; Average loss: 0.8296\n",
            "Iteration: 9050; Percent complete: 90.5%; Average loss: 0.8731\n",
            "Iteration: 9060; Percent complete: 90.6%; Average loss: 0.8433\n",
            "Iteration: 9070; Percent complete: 90.7%; Average loss: 0.8200\n",
            "Iteration: 9080; Percent complete: 90.8%; Average loss: 0.8048\n",
            "Iteration: 9090; Percent complete: 90.9%; Average loss: 0.8095\n",
            "Iteration: 9100; Percent complete: 91.0%; Average loss: 0.8077\n",
            "Iteration: 9110; Percent complete: 91.1%; Average loss: 0.8341\n",
            "Iteration: 9120; Percent complete: 91.2%; Average loss: 0.8329\n",
            "Iteration: 9130; Percent complete: 91.3%; Average loss: 0.8444\n",
            "Iteration: 9140; Percent complete: 91.4%; Average loss: 0.8422\n",
            "Iteration: 9150; Percent complete: 91.5%; Average loss: 0.8577\n",
            "Iteration: 9160; Percent complete: 91.6%; Average loss: 0.8330\n",
            "Iteration: 9170; Percent complete: 91.7%; Average loss: 0.8242\n",
            "Iteration: 9180; Percent complete: 91.8%; Average loss: 0.8256\n",
            "Iteration: 9190; Percent complete: 91.9%; Average loss: 0.7688\n",
            "Iteration: 9200; Percent complete: 92.0%; Average loss: 0.7901\n",
            "Iteration: 9210; Percent complete: 92.1%; Average loss: 0.7914\n",
            "Iteration: 9220; Percent complete: 92.2%; Average loss: 0.8266\n",
            "Iteration: 9230; Percent complete: 92.3%; Average loss: 0.8203\n",
            "Iteration: 9240; Percent complete: 92.4%; Average loss: 0.8115\n",
            "Iteration: 9250; Percent complete: 92.5%; Average loss: 0.7576\n",
            "Iteration: 9260; Percent complete: 92.6%; Average loss: 0.7852\n",
            "Iteration: 9270; Percent complete: 92.7%; Average loss: 0.7841\n",
            "Iteration: 9280; Percent complete: 92.8%; Average loss: 0.8101\n",
            "Iteration: 9290; Percent complete: 92.9%; Average loss: 0.7883\n",
            "Iteration: 9300; Percent complete: 93.0%; Average loss: 0.8087\n",
            "Iteration: 9310; Percent complete: 93.1%; Average loss: 0.8039\n",
            "Iteration: 9320; Percent complete: 93.2%; Average loss: 0.8345\n",
            "Iteration: 9330; Percent complete: 93.3%; Average loss: 0.8144\n",
            "Iteration: 9340; Percent complete: 93.4%; Average loss: 0.8080\n",
            "Iteration: 9350; Percent complete: 93.5%; Average loss: 0.7768\n",
            "Iteration: 9360; Percent complete: 93.6%; Average loss: 0.7522\n",
            "Iteration: 9370; Percent complete: 93.7%; Average loss: 0.7840\n",
            "Iteration: 9380; Percent complete: 93.8%; Average loss: 0.7517\n",
            "Iteration: 9390; Percent complete: 93.9%; Average loss: 0.8070\n",
            "Iteration: 9400; Percent complete: 94.0%; Average loss: 0.7884\n",
            "Iteration: 9410; Percent complete: 94.1%; Average loss: 0.7877\n",
            "Iteration: 9420; Percent complete: 94.2%; Average loss: 0.7382\n",
            "Iteration: 9430; Percent complete: 94.3%; Average loss: 0.7393\n",
            "Iteration: 9440; Percent complete: 94.4%; Average loss: 0.7709\n",
            "Iteration: 9450; Percent complete: 94.5%; Average loss: 0.7891\n",
            "Iteration: 9460; Percent complete: 94.6%; Average loss: 0.7620\n",
            "Iteration: 9470; Percent complete: 94.7%; Average loss: 0.7796\n",
            "Iteration: 9480; Percent complete: 94.8%; Average loss: 0.7903\n",
            "Iteration: 9490; Percent complete: 94.9%; Average loss: 0.7876\n",
            "Iteration: 9500; Percent complete: 95.0%; Average loss: 0.7800\n",
            "Iteration: 9510; Percent complete: 95.1%; Average loss: 0.7837\n",
            "Iteration: 9520; Percent complete: 95.2%; Average loss: 0.7543\n",
            "Iteration: 9530; Percent complete: 95.3%; Average loss: 0.7475\n",
            "Iteration: 9540; Percent complete: 95.4%; Average loss: 0.7590\n",
            "Iteration: 9550; Percent complete: 95.5%; Average loss: 0.7264\n",
            "Iteration: 9560; Percent complete: 95.6%; Average loss: 0.7546\n",
            "Iteration: 9570; Percent complete: 95.7%; Average loss: 0.7779\n",
            "Iteration: 9580; Percent complete: 95.8%; Average loss: 0.7746\n",
            "Iteration: 9590; Percent complete: 95.9%; Average loss: 0.7253\n",
            "Iteration: 9600; Percent complete: 96.0%; Average loss: 0.7166\n",
            "Iteration: 9610; Percent complete: 96.1%; Average loss: 0.7249\n",
            "Iteration: 9620; Percent complete: 96.2%; Average loss: 0.7366\n",
            "Iteration: 9630; Percent complete: 96.3%; Average loss: 0.7590\n",
            "Iteration: 9640; Percent complete: 96.4%; Average loss: 0.7507\n",
            "Iteration: 9650; Percent complete: 96.5%; Average loss: 0.7695\n",
            "Iteration: 9660; Percent complete: 96.6%; Average loss: 0.7530\n",
            "Iteration: 9670; Percent complete: 96.7%; Average loss: 0.7808\n",
            "Iteration: 9680; Percent complete: 96.8%; Average loss: 0.7388\n",
            "Iteration: 9690; Percent complete: 96.9%; Average loss: 0.7311\n",
            "Iteration: 9700; Percent complete: 97.0%; Average loss: 0.7426\n",
            "Iteration: 9710; Percent complete: 97.1%; Average loss: 0.6879\n",
            "Iteration: 9720; Percent complete: 97.2%; Average loss: 0.7199\n",
            "Iteration: 9730; Percent complete: 97.3%; Average loss: 0.7159\n",
            "Iteration: 9740; Percent complete: 97.4%; Average loss: 0.7630\n",
            "Iteration: 9750; Percent complete: 97.5%; Average loss: 0.7380\n",
            "Iteration: 9760; Percent complete: 97.6%; Average loss: 0.7307\n",
            "Iteration: 9770; Percent complete: 97.7%; Average loss: 0.6777\n",
            "Iteration: 9780; Percent complete: 97.8%; Average loss: 0.7088\n",
            "Iteration: 9790; Percent complete: 97.9%; Average loss: 0.6972\n",
            "Iteration: 9800; Percent complete: 98.0%; Average loss: 0.7323\n",
            "Iteration: 9810; Percent complete: 98.1%; Average loss: 0.7152\n",
            "Iteration: 9820; Percent complete: 98.2%; Average loss: 0.7298\n",
            "Iteration: 9830; Percent complete: 98.3%; Average loss: 0.7260\n",
            "Iteration: 9840; Percent complete: 98.4%; Average loss: 0.7524\n",
            "Iteration: 9850; Percent complete: 98.5%; Average loss: 0.7320\n",
            "Iteration: 9860; Percent complete: 98.6%; Average loss: 0.7252\n",
            "Iteration: 9870; Percent complete: 98.7%; Average loss: 0.6991\n",
            "Iteration: 9880; Percent complete: 98.8%; Average loss: 0.6857\n",
            "Iteration: 9890; Percent complete: 98.9%; Average loss: 0.7018\n",
            "Iteration: 9900; Percent complete: 99.0%; Average loss: 0.6709\n",
            "Iteration: 9910; Percent complete: 99.1%; Average loss: 0.7373\n",
            "Iteration: 9920; Percent complete: 99.2%; Average loss: 0.7207\n",
            "Iteration: 9930; Percent complete: 99.3%; Average loss: 0.7288\n",
            "Iteration: 9940; Percent complete: 99.4%; Average loss: 0.6682\n",
            "Iteration: 9950; Percent complete: 99.5%; Average loss: 0.6678\n",
            "Iteration: 9960; Percent complete: 99.6%; Average loss: 0.7040\n",
            "Iteration: 9970; Percent complete: 99.7%; Average loss: 0.7031\n",
            "Iteration: 9980; Percent complete: 99.8%; Average loss: 0.6857\n",
            "Iteration: 9990; Percent complete: 99.9%; Average loss: 0.6896\n",
            "Iteration: 10000; Percent complete: 100.0%; Average loss: 0.7189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIoJ0EpoEOk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "73c4a09c-c2db-4e1b-f673-f5c2d1328ed6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(lossvalues)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD6CAYAAACIyQ0UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV5f3/8dcnJ3sxQ5gh7CEKYtjUBTjQqrW2ddVZ6bfDWmvrV2z7tUq1tbWuOn7SOqpVa6WOigvFAeIMguy9ZwJC9s71++McQpITkgPk5NwJ7+fjkYc597nvk8+dW99eue7rvi5zziEiIt4VFekCRESkcQpqERGPU1CLiHicglpExOMU1CIiHqegFhHxuJCC2sxuNLPlZrbMzJ43s/hwFyYiIn7W1DhqM+sBfAQMdc6VmNm/gTecc08d6pjOnTu7zMzM5qxTRKRNW7hw4R7nXFpD70WH+BnRQIKZVQCJwI7Gds7MzCQ7O/vwqhQROYaZ2eZDvddk14dzbjtwD7AF2AnkOefmNPBDpplZtpll5+bmHk29IiJSS5NBbWYdgPOBPkB3IMnMLq+/n3NupnMuyzmXlZbWYOtdRESOQCg3EycDG51zuc65CuAlYHx4yxIRkQNCCeotwFgzSzQzAyYBK8NbloiIHBBKH/VnwCzgS2Bp4JiZYa5LREQCQhr14Zy7DbgtzLWIiEgD9GSiiIjHeSqoH5y7lg/XaGifiEhtngrqRz5Yx4J1eyJdhoiIp3gqqA1DS4OJiNTlraA2UE6LiNTlraAGlNMiInV5KqijzNSiFhGpx1NBjUG1klpEpA5PBbVFugAREQ/yVlCbRn2IiNTnsaDWzUQRkfq8FdRoeJ6ISH3eCmoznNrUIiJ1eCuoUYtaRKQ+bwW1+qhFRIJ4LKj1wIuISH3eCmrQ8DwRkXq8FdSalElEJIi3ghqN+hARqa/JoDazQWa2uNZXvpn9PBzFqEUtIhKsycVtnXOrgREAZuYDtgMvh6MYTXMqIhLscLs+JgHrnXObw1GMRn2IiAQ73KC+GHi+oTfMbJqZZZtZdm7ukS9Qqz5qEZG6Qg5qM4sFzgNebOh959xM51yWcy4rLS3tiIox9X2IiAQ5nBb12cCXzrndYSvGTDktIlLP4QT1JRyi26O5mFZ4EREJElJQm1kSMAV4KZzFaFImEZFgTQ7PA3DOFQGdwlxLYJpTERGpzWNPJmquDxGR+jwV1GiaUxGRIJ4KagMltYhIPd4Kai3FJSISxFtBjUZ9iIjU56mgjtJcHyIiQTwV1HrgRUQkmKeCGnQvUUSkPk8FtaY5FREJ5q2gBtSmFhGpy1tBraW4RESCeC+oI12EiIjHeCuoMc31ISJSj7eCWi1qEZEgHgtqjfoQEanPW0GNHngREanPW0Ftka5ARMR7vBXUaHieiEh93gpqTXMqIhIk1MVt25vZLDNbZWYrzWxcOIpRi1pEJFhIi9sCDwBvOecuMrNYIDEcxejJRBGRYE0GtZm1A04GrgJwzpUD5eEoxlDXh4hIfaF0ffQBcoEnzWyRmf3dzJLq72Rm08ws28yyc3Nzj6watahFRIKEEtTRwEjgUefciUARcEv9nZxzM51zWc65rLS0tCMrRk8miogECSWotwHbnHOfBV7Pwh/czU5zfYiIBGsyqJ1zu4CtZjYosGkSsCIcxehmoohIsFBHfVwPPBsY8bEBuDocxWhSJhGRYCEFtXNuMZAV5lrU9SEi0gCPPZmoFrWISH2eCmpQH7WISH2eCmr/XB8iIlKbt4Ia1KQWEanHW0GtPmoRkSCeCuooM63wIiJSj6eCWtOciogE81ZQ68lEEZEgngpq0KgPEZH6PBXU/ha1olpEpDZvBXWkCxAR8SBvBbX6qEVEgngrqLUUl4hIEG8FtVrUIiJBPBXUUZrrQ0QkiKeCGkNPJoqI1OOpoPZPyhTpKkREvMVTQe2LMqrUohYRqcNzQV1ZpaAWEaktpDUTzWwTUABUAZXOubCsnxgdZVRVK6hFRGoLdRVygNOcc3vCVgngi4pS14eISD2e6vpQi1pEJFioQe2AOWa20MymNbSDmU0zs2wzy87NzT2iYvx91NVHdKyISFsValBPdM6NBM4GfmJmJ9ffwTk30zmX5ZzLSktLO6Ji1KIWEQkWUlA757YH/pkDvAyMDkcxPp9RqaAWEamjyaA2syQzSznwPXAGsCwcxahFLSISLJRRH+nAy2Z2YP/nnHNvhaMYX1QUldUO5xyBnycicsxrMqidcxuA4S1QC9FR/nCuduBTTouIAB4bnucLBHVltUZ+iIgc4KmgPtCiVj+1iMhBngrqgy1qBbWIyAGeCuqaFrUmZhIRqeGpoFaLWkQkmMeC2l+O+qhFRA7yVFBHa9SHiEgQTwW1T6M+RESCeCqo42L85RSVVUW4EhER7/BUUA/umgLAsh15Ea5ERMQ7PBXUfTsnE+MzNu4pinQpIiKe4amgjooyuraLZ/u+kkiXIiLiGZ4KaoDMTkl8smEvFVrpRUQE8GBQnz64C7kFZewtLI90KSIinuC5oE5LiQMgv7QiwpWIiHiD54I6NT4GgMfnb4xwJSIi3uC9oE7wB/UL2VsjXImIiDd4LqgTY3013xeVVUawEhERb/BcUA9MT+Gq8ZkAzF+7J7LFiIh4QMhBbWY+M1tkZrPDWRDAL84YCMDmvXrwRUTkcFrUNwArw1VIbanxMXRIjGHL18Ut8eNERDwtpKA2s57AOcDfw1vOQRkdExXUIiKE3qK+H7gZOOTjgmY2zcyyzSw7Nzf3qAvL6JSkoBYRIYSgNrNzgRzn3MLG9nPOzXTOZTnnstLS0o66sE5Jsewr0tOJIiKhtKgnAOeZ2SbgX8DpZvbPsFYFJMdFU1hWiXNaREBEjm1NBrVzbrpzrqdzLhO4GHjPOXd5uAtLjo+m2kFphSZnEpFjm+fGUR+QHBcNQEGZ5vwQkWPbYQW1c+4D59y54SqmtgNBXViqpxNF5Njm2RZ1h6RYAHbll0a4EhGRyPJsUI/o1R4z+HLzvkiXIiISUZ4N6nYJMfRon8Dby3drtRcROaZ5NqgBfFHG0u15PPTeukiXIiISMZ4O6h+e3A+ApdvzIlyJiEjkeDqoLx2TwQk92/HeqhymPjBfD7+IyDHJ00ENcO3EPgCs2JnPyp0FVFZVk1eisdUicuyIjnQBTTnzuK6cP6I7ry7ewdQH59dsX3/XVHxRFsHKRERahudb1PExPh64+MSg7Xe8tjwC1YiItDzPB/UBPz2tf53X//hkc4QqERFpWa0mqH955iDu+c7wOtsyb3mdtbsLIlSRiEjLaDVBDXD+iO5B26bcN495a3I57Z4PuOO1FRoZIiJtjoUj2LKyslx2dnazfy7AW8t20iExlh15Jdz4wlcN7nPhiT2468LjiY/xhaUGEZHmZmYLnXNZDb3n+VEf9Z01rFvN94bx8xcWB+3z0qLt5JVU8PhVo1i5M5/BXVMw0wgREWmdWlXXR30XnNiDuTed0uB7c1flMGP2Cs5+YD6/++9y5q3xr+NYUFrB11riS0RakVYd1AD90pJrvr9iXO867z3+0UbAP0Lkiic+p6raccZ98xg54x2qq9WXLSKtQ6vr+mjI3JtOYW9hOaP7dATg6UMM3et36xs13z/43lp+Pnlgi9QnInI0Wn2LGvyt6gMhffnY3iTHRTMoPQWARb+dwqz/GRd0zP3vruVbjyzQKBER8bw20aKubWB6CstuP5OiskrW7C6gQ1IsWUkdG9x30Zb9PPz+OjomxXHry0tZ/H9TaJ8Y28IVi4g0rsnheWYWD8wD4vAH+yzn3G2NHRPO4XlHavv+EjbkFvL9xz9vdL+rxmfy1Meb+Ne0sYzt26mFqhORY11jw/NC6fooA053zg0HRgBnmdnY5iywJfRon8DE/p25eFQv/nntGDb98RyGdEsN2u+pjzcB8Nf31gKoa0REIq7JoHZ+hYGXMYGvVpleZsYfv30CEwd0BuCVn4znr5cET/gEsGDdXu56YyV9pr/BPz/dzOKt+wGY/tJSHgqEuIhISwjpZqKZ+cxsMZADvOOc+6yBfaaZWbaZZefm5jZ3nWERF+3jm8MPPpZ++uAudd6fOW8DAL95ZRkXPLyAXXmlPP/5Fu6Zs0brOIpIizmsR8jNrD3wMnC9c27ZofbzYh91Y/42bwOpCdF8b1QGo+98l5yCsiaPSUuJ44IR3bl16hAKyyopqaiiS0p8C1QrIm3R0fZR13DO7QfeB85qjsK84rqT+/K9URkAXD3Bv6LMwHT/gzQb/zCVe787POiY3IIy/jZ/I/fMWc13H/uU0XfO5euicjbkFvLUgo3q2xaRZhPKqI80oMI5t9/MEoA5wN3OudmHOqa1tahrc87hHFQ5R2WVIyHWh3OOPtPfaPrgWv56yYl1ulVERBpztC3qbsD7ZrYE+AJ/H/UhQ7q1MzOioowYXxQJsb6abevuPJvHr/T/DicPSWfG+cc1+jkzZq9g8db9PP3JJj5Zv7dmncd/Z29l456isJ6DiLQtrW6a00hyzjF3ZQ4T+ncmIdbH/LW5LNmWx5/fXt3ksecc343bzz+OrN+/C/jXfIwyNKufiACNt6gV1Eepsqqa/r9+E/CHb+35RJpy9rCuPHr5STX92QptkWNXs91MlGDRviimnz2Yl388Hl+UEeM7GLaXjslg9vUTuWHSgAaPfXPZLh75YB19pr/ByX9+n/zSCpZtz+PF7K0tVb6ItAJqUTezbfuK2Z1fykm9D84vUlFVzT1zVvPYhxtC/pzs30ymc3JcOEoUEQ9Si7oF9eyQWCekAWJ8UUw/ewgPBp6C/Mlp/fjtuUMb/Zys37/LzHnree2rHcyYvYK9hWVUVzv+Nm8DecUVYatfRLynzc2e52XnDe/OaYPSSImPASCzUyI78kr57Sv+Z4cSY30Ul1fV7H/XG6tqvvdFGeWV1Tz18SZmzt/AP64eTUanRJLjdAlF2jq1qFvYgZAGmDQknXF9/a3vS0ZnsOKOQz9HNHPehpoJo3ILypj64HxunuVf3Hd/cbkeaRdpw9Qci7D+XVL457VjyMrsAMDgrims2lXAvd8dzqjMjqTGx/DK4u3c9t/lQce+sXQXp/z5fTbvLSardwf+cc1obnxhMbvzS3n1pxNb+lREJEx0M7EVqKyq5oPVufzg6dB/p5/fOokuqZp7RKS10M3EVi7aF8Xkoen8/Yossnp34OUfj+ffPwxeXqy20XfN5fUlO/nX51u45qkvKCyrpKiskoffX6duEpFWRi3qViyvpILHP9rIg3P982NPP3swf3hzVYP7XjI6g1cXb6e4vIozhqYzMD2Fqydk0klDAEU8obEWtfqoW7F2CTFMO7kvZRVV3DhlIHHRUXWCunNyHBVV1eSVVPD851tqts9ZsZs5K3aTEOvjyvGZbMwt4rjuqURFmX/on/k/W0S8QS3qNuaBd9cSGx3Fj07tB/jnJ3kxexuPfri+0cmgTh/cBQPmrsoBYNMfz2mJckUkQHN9CABZv3+HPYXlIe37wMUjOH9ED/YVlbNqVwHj+mmhX5FwUlALAHnFFewvKSe/pBKH47yHFjS6f9fUeHbllwLw5FWjMPM/ZTmhf+eWKFfkmKKglkN64qON3DF7BUt+dwab9xTzzYc+avKYMX06klNQxrM/GEPn5DhiozV4SORoKajlkJxzVDv/I+rb9hUz8e736dE+gbu/fQLrcgrYtLeYl77cRn5p5SE/Y/KQLjx06UjW5RRyz5zVTBqSzvfH9m7BsxBp/RTUErJ3VuxmdJ+OdUZ9lJRXsTu/lFPv+SDkz1n02yl0SIoNQ4UibZMeeJGQTRmaHjQ0LyHWR2bnJH5zzhDGh3hT8YJHFvD6kp388sWv+OEz2ZRWVDV9kIg0SC1qOWx5xRWUV1Xz+pId/G3+RrbvL+GS0Rl1xmrXF2UwcUAak4d0YWRGB/YVl/ONAWktWLWIt+mBF2lW7RL9Le6rJvThnBO6k1NQynHd25GeGsf9767ljKHpVFRV8/7q3Jpjqh3MW5PLvDUHt3VKiiU+xsfz140lo1MiAHsLy+iQGEtUlJYlEzmgyaA2s17A00A64ICZzrkHwl2YtA5pKXGkpfgfQ790TAYL1u3hjvOH0bVdPLvySikur+RXs5bQPy2ZF+otMba3yD+m++Q/v8+TV4/ivZU5PPPpZi48sQf3fm9Ei5+LiFc12fVhZt2Abs65L80sBVgIXOCcW3GoY9T1IQ3JvOX1kPf94teTSUuJY/mOPJLjoundKSmMlYlE3lF1fTjndgI7A98XmNlKoAdwyKAWaciqGWexLqeQ9bmFxEX7mL82l2c/a7hfe9Sd73LywLSarpI/ffsEzh3ejcTYaEorqigpr9KoEjlmHNbNRDPLBOYBw5xz+fXemwZMA8jIyDhp8+bNzVeltFmfbdjLjrwSEmKiufONFWz9uoSxfTvy6YavG9z/1EFprNpZgBl8b1QvThmYxokZHVq4apHm1yzjqM0sGfgQuNM591Jj+6rrQ47EYx+u5w9vruLzWydRUFbJ7a+toLiskuzN+xo97oen9OWbJ3SnpKKKrN4dMNONSGl9jjqozSwGmA287Zy7t6n9FdRyJJxz5JdU1owqASgoreDpTzYzaUgXzrp/fkifMzA9mTk3ngJATn4p7RJjiIv2haVmkeZyVA+8mL958jiwMpSQFjlSZlYnpMG/GPBPTuvP4K6pzL5+In3Tmr6puGZ3Ibe/tpzNe4sYfddcfj97ZbhKFmkRoYz6mAjMB5YCB9ZwutU598ahjlGLWsKlrLKKvYXl7NhfQlW1Y8OeIqa/tDSkYxNjfXz0v6fz6AfrSE+N54pxmTicWtviCZrrQ9q0gtIKdueXMfneDwFYecdZTH1wfqMLJdS24JbTSY6NJiHWp5kAJWL0ZKK0aSnxMaTEx/CvaWNZsG4PCbE+7vnOcJ77bAvd28fz1/fWcemYDJ47xFDAa578gtW7CwC47ht9uOmMQcTHqJUt3qEWtbRp1dWOLV8X07tTIm8u28WPn/0ypON+deYgzhvenW89soCHLx3JmL5a4UbCS10fIgGvLt5OeWU163IKGdGrPe+s3M1LX25v9JjEWB+ZnZIY3C2FP180nHdW7OLUQV3U6pZmpaAWacSM2St4ZdH2mrlHXvvpxCZXuunfJZnoKKNnh0T+dsVJGrstR01BLRKC9bmFlFVUM7R7Kjf9+yv+8+U2LhzZg+37ShiYnsIznzb8tO1V4zOZtzaX/JIKHrz4RIZ0S9Xj7XLYFNQiR6miqprnP9/C/726PKT9n7x6FIO7pnD3m6v49TlDaZcQoxEl0iiN+hA5SjG+KC4f05vVuwoY0as9X2z6mrOGdaWwrIqfPb8oaP+rn/yi5vtXFu8A4PKxGcw4fxjF5VWYQX5JJV3bxbfYOUjrpRa1yFH601ur+Hzj16zPLWTykHSmHt+Nq5/6osF9e3VMYOvXJTWvLxzZg3u/q7m3RS1qkbC6+azBh3zvwpE9WLotj9OHdOGxDzfUCWmAl77cTkbHRF74YiunD+5C5+Q4BnVNYerx3cJdtrQialGLhEF+aQXRUUZi7MG20BMfbeSO2aFN4/7tkT0Z2j2VVTvzuXJ8JutyCpk0pAsp8TFNHyytkm4minjAtn3F/GXOGm6YNIDFW/cz9fhuzJi94pCjSRry7x+OY/PeIrq2i2f+2j386JR+GmHSRiioRTyqtKKKwb99C4Du7eL537MHM6xHOyb95cOQP2PVjLNYu7uQ43u2C1eZ0gLURy3iUfExPjb98Zyg7VeM601RWRX9uiSxLqeQX0wZyMS732/wMw4E/ZSh6azeVcCWr4t5+NKRnDY4jYQYH2ZGdbXTyu6tmFrUIq3EA++u5b531xAfE0VpRTVv/fwbXDzzU/YXVzS4vxkM79metJQ43lmxm39cM5qTeneg2jlS1dftOer6EGkDissr2fJ1Md3bJ7Axt4jhvdrz8qJt3PjCVzX7XH96fxZu3sfH6/c2+lk/mzSAB+eu5bnrxjC+X+dwly4hUFCLtFHOOd5YuouE2CjmrdnDj0/rx/Z9JXzrkY9r9kmM9XHDpAH84c1VDX7GTVMG8unGvdzzneHMXZnDzrwSfnnGIM1f0sIU1CLHmP3F5RSUVrJ1XzGJsdGM6NWezFteD/n4H53ajwFdkumXlky1c7y+ZCc3ThlIUpxua4WLbiaKHGPaJ8bSPjGWXh0Ta7bdNGUg76zczVnDupIcF80V4zI5/6GP+GpbXs0+3x/bm2c+3cyjH6wP+syNe4pIbxfPiF7tGdotlaXb85jYv3OdnyHhoRa1yDEsJ7+UU+/5gIQYH3uLyvnwV6eyZnch1z1d97/f47qnsnxHftDxmZ0S+dNFw9nydTHfOrEHryzaTmllFRePysCnUSaH5ai6PszsCeBcIMc5NyyUH6igFml9SsqrSIj1L4awr6icr7bt5zevLOPWqUMY06cjJ/3+3ZA/6/rT+wNw8egMNu8tYl9RBWcP66ohgo042qA+GSgEnlZQixxbnHM1NxVr93Ef1z2V2ddPpM/0Nw7r864c15uLR2eQFBvN3W+v4rLRGYzvr1En0Aw3E80sE5itoBY5duXklxLti6KiqpqOSbHE+KKYtXAbv3zx4PDAJb87g/veWcOTCzbVbOuQGMO+Q4z1Brh6QiY9OyRyWWAB4s4pcZw3vHs4T8WTWiSozWwaMA0gIyPjpM2bQ5+/QERat8KySnbs96+EA/4JqEorq1i9q4DvZfWiS2o8k++t+1j8+H6dDjnee8YFw1i6bT+/PHMQc5bvZs3uAm6dOqTOOpW1W/ttgVrUIhJxVzzxOX07J/HUx5s454RuPHzpyKAhgxP6d2LBuoPhHWVQXSuienVM4NdTh7Azr5TbX1vBI5eN5OxhXWsCuzU/Kq+gFhHPWLUrnwFdUvBFGX+fv4HH5m3AZ0a/Lkk8+4OxPPfZFm59eWnN/j07JLBjf0mdwK4vs1MivzvvOK568gtS46N57rqxfLVtP5eN6Y1zjuLyKs+PAVdQi4in1e/G2FNYxupdBSzeup9zju9GZXU1k++dV+eYayb04YkFGxv93D9ceDx/mbOGwrIKZv3PeKJ9xqD0FIrLq3hp0XYuHtWLGJ831rI82lEfzwOnAp2B3cBtzrnHGztGQS0izW3O8l10SY3nZ88v4pHLRjK0Wyp9b6076uRP3z6Bm/+zpNHPaZ8YUzOR1bdO7IFzjqsm9CE5zkdhWRVDu6VGZCFiPUIuIm1SWWUV0VFRPPPJJib078yA9BRW7yrgR/9cyKjMjmzaW8RNZwzirWW7mmx91zahfyeqq/0jUqY9s5Dje7Tj/33/JL4KLPhQWVXN3qJy0lObb3FiBbWIHNN27C/h5llL6NounlkLtzHj/OMY3acTZ95ftzuldmv7UK6d2IfHP/KH/sOXjuTut1Zx69TB9E1LpryymuO6px7RaBQFtYgI/lEhxRVVJAduLC7eup/iskpmLdzGZWMzOKl3R+59Zw0Pzl1bc8zVEzLrjAtvTHJcNF/+dsoRdZ1oUiYRESAqympCGmBEr/YAdZ6O/MWUgfxiykC27SsmxhdFemo8A7qk1BmJ8rNJA+jTObHOXOAAV47vHZb+bQW1iEgDenY4OCvgJaN7cdFJPdlfXE5OQRnDevjXp+yUFMfyHfmszy3kktEZjMxoH5ZaFNQiIk0wM2KjjS6p8XSpdQPx5IFpnDwwLew/3xsDCEVE5JAU1CIiHqegFhHxOAW1iIjHKahFRDxOQS0i4nEKahERj1NQi4h4XFjm+jCzXOBI1+LqDOxpxnJaA53zsUHn3PYdzfn2ds41+PRMWIL6aJhZ9qEmJmmrdM7HBp1z2xeu81XXh4iIxymoRUQ8zotBPTPSBUSAzvnYoHNu+8Jyvp7roxYRkbq82KIWEZFaFNQiIh7nmaA2s7PMbLWZrTOzWyJdT3Mxs15m9r6ZrTCz5WZ2Q2B7RzN7x8zWBv7ZIbDdzOzBwO9hiZmNjOwZHDkz85nZIjObHXjdx8w+C5zbC2YWG9geF3i9LvB+ZiTrPlJm1t7MZpnZKjNbaWbj2vp1NrMbA/9eLzOz580svq1dZzN7wsxyzGxZrW2HfV3N7MrA/mvN7MrDqcETQW1mPuBh4GxgKHCJmQ2NbFXNphK4yTk3FBgL/CRwbrcAc51zA4C5gdfg/x0MCHxNAx5t+ZKbzQ3Aylqv7wbuc871B/YB1wa2XwvsC2y/L7Bfa/QA8JZzbjAwHP+5t9nrbGY9gJ8BWc65YYAPuJi2d52fAs6qt+2wrquZdQRuA8YAo4HbDoR7SJxzEf8CxgFv13o9HZge6brCdK6vAlOA1UC3wLZuwOrA948Bl9Tav2a/1vQF9Az8C3w6MBsw/E9sRde/5sDbwLjA99GB/SzS53CY59sO2Fi/7rZ8nYEewFagY+C6zQbObIvXGcgElh3pdQUuAR6rtb3Ofk19eaJFzcELfsC2wLY2JfCn3onAZ0C6c25n4K1dQHrg+7byu7gfuBmoDrzuBOx3zlUGXtc+r5pzDryfF9i/NekD5AJPBrp7/m5mSbTh6+yc2w7cA2wBduK/bgtp29f5gMO9rkd1vb0S1G2emSUD/wF+7pzLr/2e8/8vts2MkzSzc4Ec59zCSNfSgqKBkcCjzrkTgSIO/jkMtMnr3AE4H///pLoDSQR3EbR5LXFdvRLU24FetV73DGxrE8wsBn9IP+uceymwebeZdQu83w3ICWxvC7+LCcB5ZrYJ+Bf+7o8HgPZmFh3Yp/Z51Zxz4P12wN6WLLgZbAO2Oec+C7yehT+42/J1ngxsdM7lOucqgJfwX/u2fJ0PONzrelTX2ytB/QUwIHC3OBb/DYn/RrimZmFmBjwOrHTO3Vvrrf8CB+78Xom/7/rA9isCd4/HAnm1/sRqFZxz051zPZ1zmfiv5XvOucuA94GLArvVP+cDv4uLAvu3qpanc24XsNXMBgU2TQJW0IavM/4uj7Fmlhj49/zAObfZ61zL4V7Xt4EzzKxD4C+RMwLbQhPpTvpanetTgTXAeuDXka6nGc9rIv4/i5YAiwNfU/H3zc0F1gLvAh0D+xv+ETDrgaX47zKRObUAAACJSURBVKhH/DyO4vxPBWYHvu8LfA6sA14E4gLb4wOv1wXe7xvpuo/wXEcA2YFr/QrQoa1fZ+B2YBWwDHgGiGtr1xl4Hn8ffAX+v5yuPZLrClwTOPd1wNWHU4MeIRcR8TivdH2IiMghKKhFRDxOQS0i4nEKahERj1NQi4h4nIJaRMTjFNQiIh73/wGyhTWtq584HAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoQNx3zOOVJ_"
      },
      "source": [
        "# **Bleu score Calculation** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqBXEl1FBUQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "912bb1b6-b1a5-46d4-8faf-ec510b6a08e3"
      },
      "source": [
        "'''# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
        "gram1_bleu_score = []\n",
        "gram2_bleu_score = []\n",
        "for i in range(0,len(testpairs),1):\n",
        "  \n",
        "  input_sentence = testpairs[i][0]\n",
        "  \n",
        "  reference = testpairs[i][1:]\n",
        "  templist = []\n",
        "  for k in range(len(reference)):\n",
        "    if(reference[k]!=''):\n",
        "      temp = reference[k].split(' ')\n",
        "      templist.append(temp)\n",
        "  \n",
        "  \n",
        "  input_sentence = normalizeString(input_sentence)\n",
        "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "  chencherry = SmoothingFunction()\n",
        "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
        "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
        "  gram1_bleu_score.append(score1)\n",
        "  gram2_bleu_score.append(score2)\n",
        "  if i%1000 == 0:\n",
        "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
        "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
        "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score)) '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Set dropout layers to eval mode\\nencoder.eval()\\ndecoder.eval()\\n\\n# Initialize search module\\nfrom nltk.translate.bleu_score import sentence_bleu,corpus_bleu\\nfrom nltk.translate.bleu_score import SmoothingFunction\\n\\n\\nsearcher = BeamSearchDecoder(encoder, decoder, voc, 10)\\ngram1_bleu_score = []\\ngram2_bleu_score = []\\nfor i in range(0,len(testpairs),1):\\n  \\n  input_sentence = testpairs[i][0]\\n  \\n  reference = testpairs[i][1:]\\n  templist = []\\n  for k in range(len(reference)):\\n    if(reference[k]!=\\'\\'):\\n      temp = reference[k].split(\\' \\')\\n      templist.append(temp)\\n  \\n  \\n  input_sentence = normalizeString(input_sentence)\\n  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\\n  output_words[:] = [x for x in output_words if not (x == \\'EOS\\' or x == \\'PAD\\')]\\n  chencherry = SmoothingFunction()\\n  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\\n  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \\n  gram1_bleu_score.append(score1)\\n  gram2_bleu_score.append(score2)\\n  if i%1000 == 0:\\n    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\\nprint(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \\nprint(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score)) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yqAo4s85TIzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(gram1_bleu_score)\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "cXq4ZRALoB_C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bad857d6-e25f-4cb5-9e48-ad0f65047c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import matplotlib.pyplot as plt\\n\\nplt.plot(gram1_bleu_score)\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKImFMbCn8QD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b3c17b29-f9b7-4053-941c-f1f94ddeccd7"
      },
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(gram2_bleu_score)\n",
        "plt.show()'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import matplotlib.pyplot as plt\\n\\nplt.plot(gram2_bleu_score)\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "shyaclMITIvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gajEmEGYUzIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0FwkKQpTUzE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO5NRZ0PHzTJ"
      },
      "source": [
        "# **diagnosis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IY_EHuiHyL2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "bd74d216-087c-463a-f10f-639594afed46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport csv\\nV1=[]\\nwith open(\\'/content/drive/MyDrive/Data/Mayaar.csv\\', newline=\\'\\') as f:\\n    reader = csv.reader(f)\\n    V1 = list(reader)\\n\\nprint(V1)\\n\\n\\n\\n\\ndef fD2(input_sentence):\\n    global x\\n    #print (\"chatbot: are you have any problem else?\")\\n    #text = input(\"You: \")\\n\\n    z=0\\n    #c=fD1(z,input_sentence)\\n\\n    sentences = sent_tokenize(input_sentence)\\n    for s in sentences:\\n        if (s in V1):\\n            #global x\\n            z+=1\\n            print(\"z1{}\".format(z))\\n        else:\\n            z+=0\\n            print(\"z2{}\".format(z))\\n    words = word_tokenize(input_sentence)\\n    for w in words:\\n        if (w in V1):\\n            z+=1 \\n            print(\"z3{}\".format(z))\\n        else:\\n            z+=0\\n            print(\"z4{}\".format(z))\\n    #print(\\'x: {}\\'.format(x))\\n    #fD2()\\n    if(z>1):\\n        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\\n        text = input(\"You: \")\\n        if(text==\"yes\"):\\n\\n          x+=z\\n          #print(\"chatbot: go to scal\")\\n        else:\\n          #print(\"you are fine\")\\n          z=0      \\n    #else:\\n      #print(\"you are fine\")\\n          return z\\n\\nx=0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#print (\"chatbot: Hello there! How are you doing?\")\n",
        "\n",
        "#text = input(\"You: \")\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#D ='/content/drive/MyDrive/Data/D.csv'\n",
        "\n",
        "#with open('/content/drive/MyDrive/Data/D.csv', mode='r') as f:\n",
        "    #V1 = csv.DictReader(f)\n",
        "\n",
        "'''\n",
        "file = open('/content/drive/MyDrive/Data/D.csv')\n",
        "csvreader = csv.reader(file)\n",
        "header = []\n",
        "header = next(csvreader)\n",
        "V1=[]\n",
        "for V2 in csvreader:\n",
        "    V1.append(V2)\n",
        "print(\"************\")\n",
        "print(V1)\n",
        "file.close()\n",
        "'''\n",
        "\n",
        "'''\n",
        "with open('/content/drive/MyDrive/Data/D.csv', 'rb') as file:\n",
        "    content = file.readlines()\n",
        "header = content[:1]\n",
        "rows = content[1:]\n",
        "print(header)\n",
        "print(rows)\n",
        "'''\n",
        "\n",
        "'''\n",
        "V1=[]\n",
        "with open('/content/drive/MyDrive/Data/Mayaar.csv', 'rb') as f:\n",
        "  for i in range(411):\n",
        "    v1 = f.readline()\n",
        "    V1.append(v1)\n",
        "print(V1)\n",
        "'''\n",
        "\n",
        "'''\n",
        "with open('/content/D.csv', encoding=\"utf8\", errors='ignore') as f:\n",
        "  for i in range(411):\n",
        "    v1 = f.readline()\n",
        "    V1.append(v1)\n",
        "print(V1)\n",
        "'''\n",
        "'''\n",
        "V1=[]\n",
        "with open('/content/drive/MyDrive/Data/D.txt', 'rb') as file:\n",
        "    content = file.readlines()\n",
        "header = content[:1]\n",
        "rows = content[1:]\n",
        "print(content)\n",
        "print(header)\n",
        "print(rows)\n",
        "'''\n",
        "\n",
        "'''\n",
        "data = pd.read_csv('/content/D.csv')\n",
        "data = data[(data['diagnosis'] != '[deleted]')]\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['D.csv']))# Dataset is now stored in a Pandas Dataframe\n",
        "'''\n",
        "\n",
        "#link = 'https://drive.google.com/file/d/1cPCgGPpQ6bM_1yl0vKBo0t17OH0kGOBU/view?usp=sharing'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def fD1(z,text):\n",
        "    sentences = sent_tokenize(input_sentence)\n",
        "    for s in sentences:\n",
        "        if (s in V1):\n",
        "            #global x\n",
        "            z+=1\n",
        "        else:\n",
        "            z+=0\n",
        "    words = word_tokenize(input_sentence)\n",
        "    for w in words:\n",
        "        if (w in V1):\n",
        "            z+=1 \n",
        "        else:\n",
        "            z+=0\n",
        "    return z\n",
        "'''\n",
        "'''\n",
        "V1=[]\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data/Mayaar.csv',header=1)\n",
        "#data.head()\n",
        "#data = data[(data['diagnosis'] != '[deleted]')]\n",
        "print(len(data))\n",
        "for i in range(len(data)):\n",
        "  V1.append(data[i])\n",
        "print(V1)\n",
        "'''\n",
        "'''\n",
        "import csv\n",
        "V1=[]\n",
        "with open('/content/drive/MyDrive/Data/Mayaar.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    V1 = list(reader)\n",
        "\n",
        "print(V1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fD2(input_sentence):\n",
        "    global x\n",
        "    #print (\"chatbot: are you have any problem else?\")\n",
        "    #text = input(\"You: \")\n",
        "\n",
        "    z=0\n",
        "    #c=fD1(z,input_sentence)\n",
        "\n",
        "    sentences = sent_tokenize(input_sentence)\n",
        "    for s in sentences:\n",
        "        if (s in V1):\n",
        "            #global x\n",
        "            z+=1\n",
        "            print(\"z1{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            print(\"z2{}\".format(z))\n",
        "    words = word_tokenize(input_sentence)\n",
        "    for w in words:\n",
        "        if (w in V1):\n",
        "            z+=1 \n",
        "            print(\"z3{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            print(\"z4{}\".format(z))\n",
        "    #print('x: {}'.format(x))\n",
        "    #fD2()\n",
        "    if(z>1):\n",
        "        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "        text = input(\"You: \")\n",
        "        if(text==\"yes\"):\n",
        "\n",
        "          x+=z\n",
        "          #print(\"chatbot: go to scal\")\n",
        "        else:\n",
        "          #print(\"you are fine\")\n",
        "          z=0      \n",
        "    #else:\n",
        "      #print(\"you are fine\")\n",
        "          return z\n",
        "\n",
        "x=0\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJCQUckB4CuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "f0cc4cf4-9558-4368-a478-30653babbe81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport csv\\nV1=[]\\nwith open(\\'/content/drive/MyDrive/Data/Mayaar.csv\\', newline=\\'\\') as f:\\n    reader = csv.reader(f)\\n    V1 = list(reader)\\n\\nprint(V1)\\n\\ndef fD2(input_sentence):\\n    global x\\n    z=0\\n    sentences = sent_tokenize(input_sentence)\\n    for s in sentences:\\n        if (s in V1):\\n            z+=1\\n            print(\"z1{}\".format(z))\\n        else:\\n            z+=0\\n            print(\"z2{}\".format(z))\\n    words = word_tokenize(input_sentence)\\n    for w in words:\\n        if (w in V1):\\n            z+=1 \\n            print(\"z3{}\".format(z))\\n        else:\\n            z+=0\\n            print(\"z4{}\".format(z))\\n    if(z>1):\\n        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\\n        text = input(\"You: \")\\n        if(text==\"yes\"):\\n          return z\\nx=0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "'''\n",
        "import csv\n",
        "V1=[]\n",
        "with open('/content/drive/MyDrive/Data/Mayaar.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    V1 = list(reader)\n",
        "\n",
        "print(V1)\n",
        "\n",
        "def fD2(input_sentence):\n",
        "    global x\n",
        "    z=0\n",
        "    sentences = sent_tokenize(input_sentence)\n",
        "    for s in sentences:\n",
        "        if (s in V1):\n",
        "            z+=1\n",
        "            print(\"z1{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            print(\"z2{}\".format(z))\n",
        "    words = word_tokenize(input_sentence)\n",
        "    for w in words:\n",
        "        if (w in V1):\n",
        "            z+=1 \n",
        "            print(\"z3{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            print(\"z4{}\".format(z))\n",
        "    if(z>1):\n",
        "        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "        text = input(\"You: \")\n",
        "        if(text==\"yes\"):\n",
        "          return z\n",
        "x=0\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFccmmBwtXIU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "abf5b330-476c-49ee-94cd-38238c5ad0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_csv.reader object at 0x7f182de3ff50>\n",
            "#########\n",
            "['diagnosis', 'Loss of desire to engage in normal daily activities', 'Feeling of nervousness and depression', 'feeling of nervousness', 'feeling of melancholy', 'Depression', 'nervousness', 'melancholy', 'sorrow', 'bleakness', 'isolation', 'hopelessness', 'Feeling of hopelessness', 'crying for no reason', 'Feeling of hopelessness', 'cry', 'Bouts of crying for no apparent reason', 'sleep disturbances', 'Difficulties concentrating', 'Difficulties in making decisions', 'Unintentional weight gain', 'weight gain', 'Unintentional weight loss', 'weight loss', 'uptight', 'nervous', 'Anxiety and boredom', 'boredom', 'restless', 'worried', 'Anxiety', 'Exessive sensitivity', 'Feeling of tiredness or weakness', 'Feeling of worthlessness', 'Loss of desire for sex', 'suicidal thoughts', 'suicide attempts', 'tiredness', 'weakness', 'worthlessness', 'self-murder', 'commit suicide', 'destroy oneself', 'suicide', 'make away of myself', 'make away with myself', 'attempts to commit suicide', 'Unexplained physical problems', 'Backache for no reason', 'Headaches for no reason', 'back pain', 'head pain', 'sad', 'unhappy', 'Feeling depressed', 'Feeling down', 'Feeling nervous', 'i feel nervous', 'i feel depressed', 'overtaken by the?epression', 'Feeling worthless', 'Feeling worthless or guilty', 'Feeling  guilty', 'suicide attempts', 'Loss of interest', 'Loss of interest in activities once enjoyed', 'Loss of interest in pleasure once enjoyed', 'Changes in appetite', 'Changes in weight', 'Changes in gain', 'Loss of energy', 'Trouble sleeping', 'sleeping too much', 'Difficulty concentrating', 'Difficulty making decisions', 'increased fatigue', 'feeling unhappy', 'feeling miserable', 'feeling irritable', 'feeling frustrated', 'feeling lacking in confidence', 'feeling disappointed', 'I am a failure', 'i am a failure', \"It's my fault\", 'It is my fault', 'Nothing good ever happens to me', 'I am worthless', 'i am worthless', 'There is nothing good in my life', 'Life is not worth living', 'People would be better off without me', 'frequent headaches', 'I feel so alone', 'They probably don? like me', 'point in making any effort it doesn? pay off', 'I just hate myself', 'I feel like crying all the time', 'Feeling upset', 'Feeling numb', 'Feeling despairing', 'Crying a lot', 'Losing confidence', 'Thoughts of suicide', 'Feeling restless', 'Thoughts of death', 'suicidal attempts', 'feelin depressed', 'feelin unhappy', 'feelin miserable', 'feelin frustrated', 'feelin disappointed', 'feelin worthless', 'feelin sad', 'feel more agitated', 'feelin agitated', 'my life? kinda shitty', 'feelin down', 'Trouble falling asleep', 'Trouble staying asleep', 'having little energy', 'Poor appetite', 'eating alot', 'loss of mood', 'suicidality', 'weight disorder', 'feelings of worthlessness', 'cannot think straight', 'cannot think right', 'i feel tight', 'feelin tight', 'everyone ignoring me', 'my sleeping pattern is messed up', \"i don't have a desire to eat\", \"i don't have a desire to live\", 'i want to die', 'i really want to die', 'feelin lonley', 'Feeling bad about myself', 'i would be better off dead', 'i feel like i want to hurt myself', 'dull', 'not interested', 'life is boring', 'life has no taste', 'the days are the same', 'my chest upset', 'I despair', 'I feel despair', 'my life is miserable', 'I am miserable', 'i feel miserable', 'bothered', 'I feel distressed', 'I do not sleep', 'my sleep is intermittent', 'my sleep is irregular', 'my sleep is messy', 'I have insomnia', 'I am tired no energy', 'I have no energy', 'I cannot do anything', 'tired', 'i am tired', 'exhausted', 'I do not want to eat', 'I cannot eat', 'I am not able to eat', 'I am not satisfied with myself', 'I have let myself down', 'I have let my family down', 'i feel down I am not satisfied with myself', 'I feel dissatisfied with myself', 'I feel dissatisfied', 'I am disappointed In myself', 'I disappointed my family', 'I disappointed a lot', 'I disappointed people', 'I disappointed myself', 'my thinking is scattered', 'My attention is distracted', 'I feel lethargic', 'I feel lazy', 'I am lazy', 'I have laziness', 'I want to die', 'I want to commit suicide', 'I do not want to live', 'i want to end my life', 'i want myself to die', 'I wish I had died', 'I wish I was dead', 'not amused', 'not enjoying', 'bored', 'I feel bored', 'life is boring', 'feeling hopless', 'feelin hopless', 'hopless', 'sadness', 'i feel bad', 'i feel bad about myself', \"i don't wanna live anymore\", \"i don't wanna live\", \"i don't wanna eat\", \"i don't sleep well\", \"i can't concentrate\", 'feeling down', 'the feeling of failure', 'I feel like crying all the time', \"i want to cry but i can't\", 'the feeling of depression', 'the feeling of loss', 'i have a headache for no reason ', 'i feel like i am not me anymore', 'negative thoughts', 'miserable', 'helpless', 'powerless', 'i feel like heart broken', 'i feel like heartbroken', 'had it hurt', 'is in so much pain', 'had it so painful', 'it hurts so insanely', 'had it so insanely painful', 'hate living', 'hate my life', 'at the very bottom', 'everything is hopeless', 'everything feels completely hopeless', 'have lost hope', 'I lose hope', 'hopelessness', 'I do not get anything', 'I do not get to anyone', 'I can not do anything right', 'I am not doing anything right', 'unable to think', 'unable to think', 'no longer live', 'not wanting to do anything', 'meaningless', 'nothing makes sense', 'nothing has any meaning', 'sees no meaning', 'no fun anymore', 'no profit to anything', 'do not believe in myself', 'not be social anymore', 'become antisocial', 'inner turmoil', 'nobody cares about me', 'no joy', 'likes me', 'no one like me', 'no happiness', 'nobody likes me', 'no one like me', 'no one misses me', 'no one will miss me', 'nothing feels', 'nothing interests me', 'lost interest', 'nothing to live for', 'I am a difficult person', 'unable to live', 'unconcentrated', 'i do not concentrate', \"i don't concentrate\", 'not and concentrate', 'to concentrate', 'with concentrating', 'no longer concentration', 'lost concentration', 'lose concentration', 'me far down', 'me so far down', 'tired of life', 'sad', 'live with myself', 'feeling of indifference', 'indifference', 'I? indifferent', 'blu initiative', 'locks me inside', 'lost appetite', 'no appetite', 'small appetite', 'have no appetite', 'lost motivation', 'little motivation', 'demotivated', 'the motivation is gone', 'darkest thoughts', 'the dark clouds', 'dark hole', 'dark place', 'dark thoughts', 'everything is dark', 'down', 'think negatively', 'negative thoughts', 'negative inside me', 'nervous feeling', 'nervous all the time', 'useless', 'tired', 'suicide', 'suicidal thoughts', 'pushes my friends away', 'pushes friends away', 'tired', 'I? struggling', 'struggling with me', 'I stopped participating', 'the food tastes nothing', 'me as a loser', 'sleep', 'to fall asleep', 'sleeping away', 'shuts me in', 'locked me inside', 'suicide', 'end my life', 'take my life', 'end my own life', 'take my own life', 'takes my life', 'takes my own life', 'think of death', 'completely empty', 'emptiness', 'i am empty', 'I? empty', 'sad', 'constantly tired', 'always tired', 'tear', 'i am useless', 'unmotivated', 'burnt out', 'I? exhausted', 'I? so exhausted', 'feel so exhausted', 'is just completely exhausted', 'mentally exhausted', 'I? very exhausted', 'unbearable', 'away from this world', 'does not evoke emotions anymore', 'nothing evokes emotions', 'I? worthless', 'wants to be dead', 'wanted to be dead', 'will not live', 'would not live', 'something more to live for', 'everyone hates me', 'everything is damn bad', 'everything was so damn bad', 'no appetite', 'little appetite', 'no appetite', 'end life', 'get away from it all', 'I do not care about anything', 'do not care about anything', 'did not care about anything', 'I should be happy', 'depressed', 'depression', 'distanced me from', 'die', 'death', 'died', 'dying', 'no more energy', 'empty of energy', 'little energy', 'no energy', 'have no energy', 'nergyless', 'low energy level', 'drained of energy', 'never enough energy', 'as a terrible human being', 'hurt inside me', 'i am not good enough', 'crying', 'grine', 'does not go out anymore', 'do not go out anymore', 'is in pain']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef fD2(input_sentence):\\n    global x\\n    z=0\\n    sentences = sent_tokenize(input_sentence)\\n    for s in sentences:\\n        if (s in V1):\\n            z+=1\\n            #print(\"z1{}\".format(z))\\n        else:\\n            z+=0\\n            #print(\"z2{}\".format(z))\\n    words = word_tokenize(input_sentence)\\n    for w in words:\\n        if (w in V1):\\n            z+=1 \\n            #print(\"z3{}\".format(z))\\n        else:\\n            z+=0\\n            #print(\"z4{}\".format(z))\\n    if(z>=1):\\n        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\\n        text = input(\"You: \")\\n        if(text==\"yes\" or text==\"YES\"):\\n          return z\\n        else:\\n           return 0\\n    else:\\n      return 0\\nx=0\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "V1=[]\n",
        "import csv\n",
        "with open('/content/drive/MyDrive/Datasets/ALLDataset/new_dataset.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    print(reader)\n",
        "    for row in reader:\n",
        "      for i in row:\n",
        "        V1.append(i)\n",
        "print(\"#########\")\n",
        "print(V1)\n",
        "'''\n",
        "def fD2(input_sentence):\n",
        "    global x\n",
        "    z=0\n",
        "    sentences = sent_tokenize(input_sentence)\n",
        "    for s in sentences:\n",
        "        if (s in V1):\n",
        "            z+=1\n",
        "            #print(\"z1{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            #print(\"z2{}\".format(z))\n",
        "    words = word_tokenize(input_sentence)\n",
        "    for w in words:\n",
        "        if (w in V1):\n",
        "            z+=1 \n",
        "            #print(\"z3{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            #print(\"z4{}\".format(z))\n",
        "    if(z>=1):\n",
        "        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "        text = input(\"You: \")\n",
        "        if(text==\"yes\" or text==\"YES\"):\n",
        "          return z\n",
        "        else:\n",
        "           return 0\n",
        "    else:\n",
        "      return 0\n",
        "x=0\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AELvsv8UFZjT"
      },
      "source": [
        "VADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pbizV22Fbwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bab94b4-1205-4592-cd23-ff36b5ff162c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 4.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U3YipzwFcAI"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#def text_sentiment_vader(text):\n",
        "# vs = analyzer.polarity_scores(text)\n",
        "# return int(vs.get(\"compound\"))\n",
        " \n",
        "## predictions = df_test.tweet.map(lambda x : text_sentiment_vader(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv-ZYv4WxeMK"
      },
      "outputs": [],
      "source": [
        "#input_sentence = input('You :   ')\n",
        "#vs = analyzer.polarity_scores(input_sentence)\n",
        "#print(vs)\n",
        "\n",
        "#vs['neg']*100, \"% Negative\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9pw4hoP4sb9"
      },
      "outputs": [],
      "source": [
        "##text = input('You :   ')\n",
        "#v=text_sentiment_vader(input_sentence)\n",
        "#print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He1jQktAP5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "36a044f2-33fa-4233-8344-da486534f932"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scale=0\\n#input_sentence = input(\\'You :   \\')\\n#vs = analyzer.polarity_scores(input_sentence)\\ndef scaleD(input_sentence):\\n  global scale\\n  vs = analyzer.polarity_scores(input_sentence)\\n  ##print(vs)\\n  n=vs[\\'neg\\']\\n  neu=vs[\\'neu\\']\\n  p=vs[\\'pos\\']\\n  if(n>=0.75):\\n    scale+=3\\n  elif((n>=0.45)&(n<0.75)):\\n    scale+=2\\n  elif((n<0.45)&(n!=0)):\\n    scale+=1\\n  else:\\n    scale+=0\\n  return scale\\ndef Rscale(scale):\\n  if(scale<=10):\\n    print(\"These ups and downs are considered normal ---> {}\".format(scale))\\n  elif((scale>=11)&(scale<=16)):\\n    print(\"Mild mood disturbance ---> {}\".format(scale))\\n  elif((scale>=17)&(scale<=20)):\\n    print(\"Borderline clinical depression ---> {}\".format(scale))\\n  elif((scale>=21)&(scale<=30)):\\n    print(\"Moderate depression ---> {}\".format(scale))\\n  elif((scale>=31)&(scale<=40)):\\n    print(\"Severe depression ---> {}\".format(scale))\\n  else:\\n    print(\"Extreme depression ---> {}\".format(scale))\\n\\n\\n\\n  #print(scale)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "'''scale=0\n",
        "#input_sentence = input('You :   ')\n",
        "#vs = analyzer.polarity_scores(input_sentence)\n",
        "def scaleD(input_sentence):\n",
        "  global scale\n",
        "  vs = analyzer.polarity_scores(input_sentence)\n",
        "  ##print(vs)\n",
        "  n=vs['neg']\n",
        "  neu=vs['neu']\n",
        "  p=vs['pos']\n",
        "  if(n>=0.75):\n",
        "    scale+=3\n",
        "  elif((n>=0.45)&(n<0.75)):\n",
        "    scale+=2\n",
        "  elif((n<0.45)&(n!=0)):\n",
        "    scale+=1\n",
        "  else:\n",
        "    scale+=0\n",
        "  return scale\n",
        "def Rscale(scale):\n",
        "  if(scale<=10):\n",
        "    print(\"These ups and downs are considered normal ---> {}\".format(scale))\n",
        "  elif((scale>=11)&(scale<=16)):\n",
        "    print(\"Mild mood disturbance ---> {}\".format(scale))\n",
        "  elif((scale>=17)&(scale<=20)):\n",
        "    print(\"Borderline clinical depression ---> {}\".format(scale))\n",
        "  elif((scale>=21)&(scale<=30)):\n",
        "    print(\"Moderate depression ---> {}\".format(scale))\n",
        "  elif((scale>=31)&(scale<=40)):\n",
        "    print(\"Severe depression ---> {}\".format(scale))\n",
        "  else:\n",
        "    print(\"Extreme depression ---> {}\".format(scale))\n",
        "\n",
        "\n",
        "\n",
        "  #print(scale)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Dep=0\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    global Dep\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('You :   ')\n",
        "\n",
        "            Dep+=fD2(input_sentence)\n",
        "            #vs = analyzer.polarity_scores(input_sentence)\n",
        "            mm=scaleD(input_sentence)\n",
        "            #print(\"Scale.{}\".format(mm))\n",
        "            \n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit' or input_sentence == 'bye' : break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot :   ', ' '.join(output_words))\n",
        "            \n",
        "            #print('Diagnos {}'.format(Dep))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")\n",
        "    if(Dep>5):\n",
        "      Rscale(mm)\n",
        "      print('Diagnos {}'.format(Dep))\n",
        "    else:\n",
        "      print(\" >>  your psychological state is great, you are fine. ^^ \")'''"
      ],
      "metadata": {
        "id": "pta24LTEUzCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "12afb5ae-5df0-4ffd-f6e4-1d4a35df6003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dep=0\\ndef evaluateInput(encoder, decoder, searcher, voc):\\n    input_sentence = \\'\\'\\n    global Dep\\n    while(1):\\n        try:\\n            # Get input sentence\\n            input_sentence = input(\\'You :   \\')\\n\\n            Dep+=fD2(input_sentence)\\n            #vs = analyzer.polarity_scores(input_sentence)\\n            mm=scaleD(input_sentence)\\n            #print(\"Scale.{}\".format(mm))\\n            \\n            # Check if it is quit case\\n            if input_sentence == \\'q\\' or input_sentence == \\'quit\\' or input_sentence == \\'bye\\' : break\\n            # Normalize sentence\\n            input_sentence = normalizeString(input_sentence)\\n            # Evaluate sentence\\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\\n            # Format and print response sentence\\n            output_words[:] = [x for x in output_words if not (x == \\'EOS\\' or x == \\'PAD\\')]\\n            print(\\'Bot :   \\', \\' \\'.join(output_words))\\n            \\n            #print(\\'Diagnos {}\\'.format(Dep))\\n\\n        except KeyError:\\n            print(\"Error: Encountered unknown word.\")\\n    if(Dep>5):\\n      Rscale(mm)\\n      print(\\'Diagnos {}\\'.format(Dep))\\n    else:\\n      print(\" >>  your psychological state is great, you are fine. ^^ \")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def fD2(input_sentence):\n",
        "    global x\n",
        "    z=0\n",
        "    sentences = sent_tokenize(input_sentence)\n",
        "    for s in sentences:\n",
        "        if (s in V1):\n",
        "            z+=1\n",
        "            #print(\"z1{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            #print(\"z2{}\".format(z))\n",
        "    words = word_tokenize(input_sentence)\n",
        "    for w in words:\n",
        "        if (w in V1):\n",
        "            z+=1 \n",
        "            #print(\"z3{}\".format(z))\n",
        "        else:\n",
        "            z+=0\n",
        "            #print(\"z4{}\".format(z))\n",
        "    if(z>=1):\n",
        "        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "        text = input(\"You: \")\n",
        "        if(text==\"yes\" or text==\"YES\"):\n",
        "          return z\n",
        "        else:\n",
        "           return 0\n",
        "    else:\n",
        "      return 0\n",
        "x=0\n",
        "'''"
      ],
      "metadata": {
        "id": "Zo-_fWdPIEaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "36a10243-7a92-4f82-f7da-8a1a431c73d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def fD2(input_sentence):\\n    global x\\n    z=0\\n    sentences = sent_tokenize(input_sentence)\\n    for s in sentences:\\n        if (s in V1):\\n            z+=1\\n            #print(\"z1{}\".format(z))\\n        else:\\n            z+=0\\n            #print(\"z2{}\".format(z))\\n    words = word_tokenize(input_sentence)\\n    for w in words:\\n        if (w in V1):\\n            z+=1 \\n            #print(\"z3{}\".format(z))\\n        else:\\n            z+=0\\n            #print(\"z4{}\".format(z))\\n    if(z>=1):\\n        print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\\n        text = input(\"You: \")\\n        if(text==\"yes\" or text==\"YES\"):\\n          return z\\n        else:\\n           return 0\\n    else:\\n      return 0\\nx=0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''scale=0\n",
        "#input_sentence = input('You :   ')\n",
        "#vs = analyzer.polarity_scores(input_sentence)\n",
        "def scaleD(input_sentence):\n",
        "  global scale\n",
        "  vs = analyzer.polarity_scores(input_sentence)\n",
        "  ##print(vs)\n",
        "  n=vs['neg']\n",
        "  neu=vs['neu']\n",
        "  p=vs['pos']\n",
        "  if(n>=0.75):\n",
        "    scale+=3\n",
        "  elif((n>=0.45)&(n<0.75)):\n",
        "    scale+=2\n",
        "  elif((n<0.45)&(n!=0)):\n",
        "    scale+=1\n",
        "  else:\n",
        "    scale+=0\n",
        "  return scale\n",
        "def Rscale(scale):\n",
        "  if(scale<=10):\n",
        "    print(\"These ups and downs are considered normal ---> {}\".format(scale))\n",
        "  elif((scale>=11)&(scale<=16)):\n",
        "    print(\"Mild mood disturbance ---> {}\".format(scale))\n",
        "  elif((scale>=17)&(scale<=20)):\n",
        "    print(\"Borderline clinical depression ---> {}\".format(scale))\n",
        "  elif((scale>=21)&(scale<=30)):\n",
        "    print(\"Moderate depression ---> {}\".format(scale))\n",
        "  elif((scale>=31)&(scale<=40)):\n",
        "    print(\"Severe depression ---> {}\".format(scale))\n",
        "  else:\n",
        "    print(\"Extreme depression ---> {}\".format(scale))\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "YhDJ3DxKyVKX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "32f3eb72-680e-4d26-ac0b-2d4c39d216c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scale=0\\n#input_sentence = input(\\'You :   \\')\\n#vs = analyzer.polarity_scores(input_sentence)\\ndef scaleD(input_sentence):\\n  global scale\\n  vs = analyzer.polarity_scores(input_sentence)\\n  ##print(vs)\\n  n=vs[\\'neg\\']\\n  neu=vs[\\'neu\\']\\n  p=vs[\\'pos\\']\\n  if(n>=0.75):\\n    scale+=3\\n  elif((n>=0.45)&(n<0.75)):\\n    scale+=2\\n  elif((n<0.45)&(n!=0)):\\n    scale+=1\\n  else:\\n    scale+=0\\n  return scale\\ndef Rscale(scale):\\n  if(scale<=10):\\n    print(\"These ups and downs are considered normal ---> {}\".format(scale))\\n  elif((scale>=11)&(scale<=16)):\\n    print(\"Mild mood disturbance ---> {}\".format(scale))\\n  elif((scale>=17)&(scale<=20)):\\n    print(\"Borderline clinical depression ---> {}\".format(scale))\\n  elif((scale>=21)&(scale<=30)):\\n    print(\"Moderate depression ---> {}\".format(scale))\\n  elif((scale>=31)&(scale<=40)):\\n    print(\"Severe depression ---> {}\".format(scale))\\n  else:\\n    print(\"Extreme depression ---> {}\".format(scale))\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbDKWSfpfLpU"
      },
      "source": [
        "# **Chatting with BOT / Evaluation**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''scale=0\n",
        "Dep=0\n",
        "z=0\n",
        "con=0\n",
        "status = False\n",
        "executed_word = list()\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "  global Dep\n",
        "  global status\n",
        "  global scale\n",
        "  global z\n",
        "    #input_sentence =str(request.args['msg'])    \n",
        "   # input_sentence = request.args['msg']\n",
        "  input_sentence=\"\"     \n",
        "  willAnalysisWord  = \"\"\n",
        "  while(1):\n",
        "    try:\n",
        "      input_sentence=input('You :   ')\n",
        "      executed_word.append(input_sentence)\n",
        "\n",
        "      sentences = sent_tokenize(input_sentence)\n",
        "      for s in sentences:\n",
        "        if (s in V1):\n",
        "          z+=1\n",
        "            \n",
        "        else:\n",
        "          z+=0\n",
        "            \n",
        "      words = word_tokenize(input_sentence)\n",
        "      for w in words:\n",
        "        if (w in V1):\n",
        "          z+=1 \n",
        "                \n",
        "        else:\n",
        "          z+=0\n",
        "                \n",
        "                  \n",
        "            \n",
        "                    \n",
        "      if(z>=1 or status):\n",
        "        if(status==False):\n",
        "          status =True  \n",
        "          print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "          #print(\"--------------------- \" + str(executed_word[-1].lower().strip().startswith(\"y\")))\n",
        "        if(executed_word[-1].lower().strip().startswith(\"y\")):\n",
        "          status =False\n",
        "          Dep+= z\n",
        "                          \n",
        "                          \n",
        "          willAnalysisWord = executed_word[-2]\n",
        "          print(\"TheWillAnalysis1= \"+willAnalysisWord)                  \n",
        "\n",
        "\n",
        "        else:\n",
        "          willAnalysisWord=\"no\"\n",
        "                      \n",
        "      else:\n",
        "        willAnalysisWord=executed_word[-1]\n",
        "                  \n",
        "                  \n",
        "      print(\"TheWillAnalysis2= \"+willAnalysisWord)        \n",
        "      vs = analyzer.polarity_scores(willAnalysisWord)\n",
        "                    \n",
        "      n=vs['neg']\n",
        "      neu=vs['neu']\n",
        "      p=vs['pos']\n",
        "      if(n>=0.75):\n",
        "        scale+=3\n",
        "      elif((n>=0.45)&(n<0.75)):\n",
        "        scale+=2\n",
        "      elif((n<0.45)&(n!=0)):\n",
        "        scale+=1\n",
        "      else:\n",
        "        scale+=0\n",
        "              \n",
        "      mm=scale\n",
        "                    ### END FUN ###\n",
        "      z=0\n",
        "                    #print(\"Scale.{}\".format(mm))\n",
        "                  \n",
        "                    # Check if it is quit case\n",
        "      print(\"the input= \"+input_sentence)\n",
        "      if input_sentence == 'q' or input_sentence == 'quit' or input_sentence == 'bye' : break\n",
        "                    # Normalize sentence\n",
        "      input_sentence = normalizeString(input_sentence)\n",
        "                    # Evaluate sentence\n",
        "      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "                    # Format and print response sentence\n",
        "      output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "      print('Bot :   ', ' '.join(output_words))\n",
        "                  \n",
        "                    #print('Diagnos {}'.format(Dep))\n",
        "      \n",
        "      \n",
        "    except KeyError:\n",
        "      print(\"Error: Encountered unknown word.\")\n",
        "  if(Dep>5):\n",
        "              ## DONE ## Rscale(mm)\n",
        "    print(\"iam dep=5\")\n",
        "              ### Rscale() ###\n",
        "    if(mm<=10):\n",
        "      print(\"These ups and downs are considered normal ---> {}\".format(mm))\n",
        "    elif((mm>=11)&(mm<=16)):\n",
        "      print(\"Mild mood disturbance ---> {}\".format(mm))\n",
        "    elif((mm>=17)&(mm<=20)):\n",
        "      print(\"Borderline clinical depression ---> {}\".format(mm))\n",
        "    elif((mm>=21)&(mm<=30)):\n",
        "      print(\"Moderate depression ---> {}\".format(mm))\n",
        "    elif((mm>=31)&(mm<=40)):\n",
        "      print(\"Severe depression ---> {}\".format(mm))\n",
        "    else:\n",
        "      print(\"Extreme depression ---> {}\".format(mm))\n",
        "              ### END FUN ###\n",
        "      \n",
        "    print('Diagnos {}'.format(Dep))\n",
        "  else:\n",
        "    print(\" >>  your psychological state is great, you are fine. ^^ \")'''"
      ],
      "metadata": {
        "id": "5OxdSBGZBZsD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "de11dcd1-5c88-40c9-f5e8-e62230092ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scale=0\\nDep=0\\nz=0\\ncon=0\\nstatus = False\\nexecuted_word = list()\\ndef evaluateInput(encoder, decoder, searcher, voc):\\n  global Dep\\n  global status\\n  global scale\\n  global z\\n    #input_sentence =str(request.args[\\'msg\\'])    \\n   # input_sentence = request.args[\\'msg\\']\\n  input_sentence=\"\"     \\n  willAnalysisWord  = \"\"\\n  while(1):\\n    try:\\n      input_sentence=input(\\'You :   \\')\\n      executed_word.append(input_sentence)\\n\\n      sentences = sent_tokenize(input_sentence)\\n      for s in sentences:\\n        if (s in V1):\\n          z+=1\\n            \\n        else:\\n          z+=0\\n            \\n      words = word_tokenize(input_sentence)\\n      for w in words:\\n        if (w in V1):\\n          z+=1 \\n                \\n        else:\\n          z+=0\\n                \\n                  \\n            \\n                    \\n      if(z>=1 or status):\\n        if(status==False):\\n          status =True  \\n          print(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\\n          #print(\"--------------------- \" + str(executed_word[-1].lower().strip().startswith(\"y\")))\\n        if(executed_word[-1].lower().strip().startswith(\"y\")):\\n          status =False\\n          Dep+= z\\n                          \\n                          \\n          willAnalysisWord = executed_word[-2]\\n          print(\"TheWillAnalysis1= \"+willAnalysisWord)                  \\n\\n\\n        else:\\n          willAnalysisWord=\"no\"\\n                      \\n      else:\\n        willAnalysisWord=executed_word[-1]\\n                  \\n                  \\n      print(\"TheWillAnalysis2= \"+willAnalysisWord)        \\n      vs = analyzer.polarity_scores(willAnalysisWord)\\n                    \\n      n=vs[\\'neg\\']\\n      neu=vs[\\'neu\\']\\n      p=vs[\\'pos\\']\\n      if(n>=0.75):\\n        scale+=3\\n      elif((n>=0.45)&(n<0.75)):\\n        scale+=2\\n      elif((n<0.45)&(n!=0)):\\n        scale+=1\\n      else:\\n        scale+=0\\n              \\n      mm=scale\\n                    ### END FUN ###\\n      z=0\\n                    #print(\"Scale.{}\".format(mm))\\n                  \\n                    # Check if it is quit case\\n      print(\"the input= \"+input_sentence)\\n      if input_sentence == \\'q\\' or input_sentence == \\'quit\\' or input_sentence == \\'bye\\' : break\\n                    # Normalize sentence\\n      input_sentence = normalizeString(input_sentence)\\n                    # Evaluate sentence\\n      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\\n                    # Format and print response sentence\\n      output_words[:] = [x for x in output_words if not (x == \\'EOS\\' or x == \\'PAD\\')]\\n      print(\\'Bot :   \\', \\' \\'.join(output_words))\\n                  \\n                    #print(\\'Diagnos {}\\'.format(Dep))\\n      \\n      \\n    except KeyError:\\n      print(\"Error: Encountered unknown word.\")\\n  if(Dep>5):\\n              ## DONE ## Rscale(mm)\\n    print(\"iam dep=5\")\\n              ### Rscale() ###\\n    if(mm<=10):\\n      print(\"These ups and downs are considered normal ---> {}\".format(mm))\\n    elif((mm>=11)&(mm<=16)):\\n      print(\"Mild mood disturbance ---> {}\".format(mm))\\n    elif((mm>=17)&(mm<=20)):\\n      print(\"Borderline clinical depression ---> {}\".format(mm))\\n    elif((mm>=21)&(mm<=30)):\\n      print(\"Moderate depression ---> {}\".format(mm))\\n    elif((mm>=31)&(mm<=40)):\\n      print(\"Severe depression ---> {}\".format(mm))\\n    else:\\n      print(\"Extreme depression ---> {}\".format(mm))\\n              ### END FUN ###\\n      \\n    print(\\'Diagnos {}\\'.format(Dep))\\n  else:\\n    print(\" >>  your psychological state is great, you are fine. ^^ \")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdB43yBKUULm"
      },
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
        "#evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok config add-authtoken 2B8yu5knwniusufYLnquMje1plh_8cNfEB9WXD4KbJCbmyCT\n",
        "!pip install pyngrok           "
      ],
      "metadata": {
        "id": "VfuI1tFkj90N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e1b595-89dc-4ffd-b778-ff192268ee6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.6.15)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Hit:9 http://ppa.launchpad.net/alessandro-strada/ppa/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [1,262 B]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 274 kB in 4s (76.7 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 5,464 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.0.6 [5,464 kB]\n",
            "Fetched 5,464 kB in 1s (6,103 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 155644 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/ngrok_3.0.6_amd64.deb ...\n",
            "Unpacking ngrok (3.0.6) ...\n",
            "Setting up ngrok (3.0.6) ...\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=492ff409fd5627b268858679eee18ec871314424b92e52b628a656c72440dea1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "port_no = 5000"
      ],
      "metadata": {
        "id": "9weY8_vJkBwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale=0\n",
        "Dep=0\n",
        "z=0\n",
        "con=0\n",
        "status = False\n",
        "executed_word = list()\n",
        "import json\n",
        "from flask import Flask, request\n",
        "#from flask_ngrok import run_with_ngrok\n",
        "from pyngrok import ngrok\n",
        "app = Flask(__name__)\n",
        "ngrok.set_auth_token(\"2B8yu5knwniusufYLnquMje1plh_8cNfEB9WXD4KbJCbmyCT\")\n",
        "public_url =  ngrok.connect(port_no).public_url\n",
        "\n",
        "@app.route('/api')\n",
        "def get_bot_reponse():\n",
        "  d={}\n",
        "  \n",
        "\n",
        "\n",
        "  user_text=  request.args['msg']\n",
        "  def evaluateInput(encoder, decoder, searcher, voc,input_sentence):\n",
        "    global Dep\n",
        "    global status\n",
        "    global scale\n",
        "    global z\n",
        "    #input_sentence =str(request.args['msg'])    \n",
        "   # input_sentence = request.args['msg']\n",
        "         \n",
        "    willAnalysisWord  = \"\"\n",
        "    while(1):\n",
        "      try:\n",
        "        executed_word.append(input_sentence)\n",
        "\n",
        "        sentences = sent_tokenize(input_sentence)\n",
        "        for s in sentences:\n",
        "          if (s in V1):\n",
        "            z+=1\n",
        "            \n",
        "          else:\n",
        "            z+=0\n",
        "            \n",
        "        words = word_tokenize(input_sentence)\n",
        "        for w in words:\n",
        "          if (w in V1):\n",
        "            z+=1 \n",
        "                \n",
        "          else:\n",
        "            z+=0\n",
        "                \n",
        "                  \n",
        "            \n",
        "                    \n",
        "        if(z>=1 or status):\n",
        "          if(status==False):\n",
        "            status =True  \n",
        "            return(\"Are the Symptoms from 6 months?  ( YES or NO ) \")\n",
        "          #print(\"--------------------- \" + str(executed_word[-1].lower().strip().startswith(\"y\")))\n",
        "          if(executed_word[-1].lower().strip().startswith(\"yes\")):\n",
        "            status =False\n",
        "            Dep+= z\n",
        "                          \n",
        "                          \n",
        "            willAnalysisWord = executed_word[-2]\n",
        "                          \n",
        "\n",
        "\n",
        "          else:\n",
        "            willAnalysisWord=\"no\"\n",
        "                      \n",
        "        else:\n",
        "          willAnalysisWord=executed_word[-1]\n",
        "                  \n",
        "                  \n",
        "              \n",
        "        vs = analyzer.polarity_scores(willAnalysisWord)\n",
        "                    \n",
        "        n=vs['neg']\n",
        "        neu=vs['neu']\n",
        "        p=vs['pos']\n",
        "        if(n>=0.75):\n",
        "          scale+=3\n",
        "        elif((n>=0.45)&(n<0.75)):\n",
        "          scale+=2\n",
        "        elif((n<0.45)&(n!=0)):\n",
        "          scale+=1\n",
        "        else:\n",
        "          scale+=0\n",
        "              \n",
        "        mm=scale\n",
        "                    ### END FUN ###\n",
        "        z=0\n",
        "                    #print(\"Scale.{}\".format(mm))\n",
        "                  \n",
        "                    # Check if it is quit case\n",
        "        print(\"the input= \"+input_sentence)\n",
        "        if input_sentence == 'q' or input_sentence == 'quit' or input_sentence == 'bye' : break\n",
        "                    # Normalize sentence\n",
        "        input_sentence = normalizeString(willAnalysisWord)\n",
        "                    # Evaluate sentence\n",
        "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "                    # Format and print response sentence\n",
        "        output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "        return(' '.join(output_words))\n",
        "                  \n",
        "                    #print('Diagnos {}'.format(Dep))\n",
        "      \n",
        "      \n",
        "      except KeyError:\n",
        "        return(\"Error: Encountered unknown word.\")\n",
        "    if(Dep>5):\n",
        "              ## DONE ## Rscale(mm)\n",
        "      print(\"iam dep=5\")\n",
        "              ### Rscale() ###\n",
        "      if(mm<=10):\n",
        "        return(\"These ups and downs are considered normal ---> {}\".format(mm))\n",
        "      elif((mm>=11)&(mm<=16)):\n",
        "        return(\"Mild mood disturbance ---> {}\".format(mm))\n",
        "      elif((mm>=17)&(mm<=20)):\n",
        "        return(\"Borderline clinical depression ---> {}\".format(mm))\n",
        "      elif((mm>=21)&(mm<=30)):\n",
        "        return(\"Moderate depression ---> {}\".format(mm))\n",
        "      elif((mm>=31)&(mm<=40)):\n",
        "        return(\"Severe depression ---> {}\".format(mm))\n",
        "      else:\n",
        "        return(\"Extreme depression ---> {}\".format(mm))\n",
        "              ### END FUN ###\n",
        "      \n",
        "      return('Diagnos {}'.format(Dep))\n",
        "    else:\n",
        "      return(\" >>  your psychological state is great, you are fine. ^^ \")\n",
        "\n",
        "  #answer=str(evaluateInput(encoder, decoder, searcher, voc))\n",
        "  answer=evaluateInput(encoder, decoder, searcher, voc,user_text)\n",
        "    \n",
        "  d=answer\n",
        "  #d=\" \".join( d['output'])\n",
        "  print(d)\n",
        "  #print( d['output'])\n",
        "  #stri  =json.dumps( d['output'])\n",
        "  #print(\"Stri \"+ stri)\n",
        "  #listToStr = ' '.join([(elem) for elem in stri])\n",
        "  #listToStr = ' '.join(map(str, stri))\n",
        "  '''d=d['listToStr'].replace(',', '')\n",
        "  d=d.replace(' ', '')\n",
        "  d=d.replace('[', '')\n",
        "  d=d.replace(']', '')\n",
        "  d=d.replace(\"'\", ' ')\n",
        "  d=d.replace('\"', '')\n",
        "  d=d.replace('.', '')  \n",
        "  #print(\"final mess \"+d)'''\n",
        "  #d=d.replace(' ', '')\n",
        "  \n",
        "  return (d)\n",
        "  \n",
        "\n",
        "print(f\"To access the link please click {public_url}\")\n",
        "#if __name__ == \"__main__\":\n",
        "#app.run(debug=True)\n",
        "app.run(port=port_no)\n",
        "    "
      ],
      "metadata": {
        "id": "IPDkPUSMkDbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdcd7be-addb-4481-ed38-c5e1ad218b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To access the link please click http://ecd5-35-225-119-205.ngrok.io\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input= hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:57:46] \"\u001b[37mGET /api?msg=hi HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi . . . . . rose . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:57:47] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [01/Jul/2022 18:57:57] \"\u001b[37mGET /api?msg=i%20am%20feeling%20sad HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:03] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yeah . . . \n",
            "the input= i need some help\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:15] \"\u001b[37mGET /api?msg=i%20need%20some%20help HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "young one \n",
            "the input= How are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:24] \"\u001b[37mGET /api?msg=How%20are%20you? HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fine . how be you ? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:36] \"\u001b[37mGET /api?msg=i%20want%20to%20cry HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:41] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i want to go . \n",
            "the input= i am feeling the life is end\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:58:59] \"\u001b[37mGET /api?msg=i%20am%20feeling%20the%20life%20is%20end HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bullshit . \n",
            "the input= i hate my self\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:59:15] \"\u001b[37mGET /api?msg=i%20hate%20my%20self HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you do not even know you . \n",
            "the input= i want kill my self\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:59:32] \"\u001b[37mGET /api?msg=i%20want%20kill%20my%20self HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that be a deal . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 18:59:51] \"\u001b[37mGET /api?msg=i%20need%20to%20cry%20and%20stay%20alone HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:00:00] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where be you go ? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:00:52] \"\u001b[37mGET /api?msg=i%20will%20go%20to%20any%20where,%20i%20am%20feeling%20sad HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:00:58] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:01:07] \"\u001b[37mGET /api?msg=i%20will%20cry HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:01:12] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no you will not . \n",
            "the input= are you my friend?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:02:07] \"\u001b[37mGET /api?msg=are%20you%20my%20friend? HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes . \n",
            "the input= i hate you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:02:14] \"\u001b[37mGET /api?msg=i%20hate%20you HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i do not know . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:02:29] \"\u001b[37mGET /api?msg=iam%20feeling%20depressed HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:02:34] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input= yes\n",
            "Error: Encountered unknown word.\n",
            "the input= hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:03] \"\u001b[37mGET /api?msg=hi HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi . . . . . rose . \n",
            "the input= how are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:11] \"\u001b[37mGET /api?msg=how%20are%20you? HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fine . how be you ? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:25] \"\u001b[37mGET /api?msg=i%20am%20not%20fine%20,%20i%20need%20to%20cry HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:28] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fine ! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:44] \"\u001b[37mGET /api?msg=no%20,%20i%20want%20to%20cry HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are the Symptoms from 6 months?  ( YES or NO ) \n",
            "the input= yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:48] \"\u001b[37mGET /api?msg=yes HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no you do not . \n",
            "the input= i hate my self\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:07:56] \"\u001b[37mGET /api?msg=i%20hate%20my%20self HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you do not even know you . \n",
            "the input= are you my friend?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:08:19] \"\u001b[37mGET /api?msg=are%20you%20my%20friend? HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes . \n",
            "the input= okay, thank you bye\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:08:33] \"\u001b[37mGET /api?msg=okay,%20thank%20you%20bye HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thank you . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [01/Jul/2022 19:08:37] \"\u001b[37mGET /api?msg=q HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the input= q\n",
            "iam dep=5\n",
            "Moderate depression ---> 22\n"
          ]
        }
      ]
    }
  ]
}